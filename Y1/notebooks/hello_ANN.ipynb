{"cells": [{"cell_type": "markdown", "metadata": {"id": "7LMWb49iIPNI"}, "source": ["# Cycle 1 - Introduction to neural networks exercices\n", "\n", "## Rodolphe Cledassou school\n", "\n", "> Alexandre Boucaud and Marc Huertas-Company"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Native PyTorch setup (replaces TF/Keras)\n", "import math, random, numpy as np\n", "import torch, torch.nn as nn, torch.nn.functional as F\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "\n", "sns.set_theme(context='notebook', style='whitegrid', font='sans-serif', font_scale=1.2, rc={\"lines.linewidth\": 2})\n", "\n", "device = torch.device(\"mps\" if torch.backends.mps.is_available() else\n", "                      \"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "print(\"Using device:\", device)\n", "\n", "torch.manual_seed(0); np.random.seed(0); random.seed(0)\n", "\n", "# Small helpers\n", "def to_tensor(x):\n", "    x = np.asarray(x, dtype=np.float32)\n", "    return torch.from_numpy(x).to(device).view(-1, 1)\n", "\n", "@torch.no_grad()\n", "def predict(model, x_np):\n", "    x_t = to_tensor(x_np)\n", "    y_t = model(x_t).squeeze(1)\n", "    return y_t.detach().cpu().numpy()\n", "\n", "class History:\n", "    pass\n", "\n", "def train_regressor(model, x_np, y_np, epochs=200, lr=1e-2, bs=None, verbose=False):\n", "    model.to(device).train()\n", "    x_t, y_t = to_tensor(x_np), to_tensor(y_np).squeeze(1)\n", "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n", "    loss_fn = nn.MSELoss()\n", "    losses = []\n", "    N = x_t.size(0)\n", "    for ep in range(epochs):\n", "        if bs is None:\n", "            # full-batch\n", "            opt.zero_grad()\n", "            pred = model(x_t).squeeze(1)\n", "            loss = loss_fn(pred, y_t)\n", "            loss.backward(); opt.step()\n", "        else:\n", "            # mini-batch\n", "            idx = torch.randperm(N)\n", "            for s in range(0, N, bs):\n", "                b = idx[s:s+bs]\n", "                xb, yb = x_t[b], y_t[b]\n", "                opt.zero_grad()\n", "                pred = model(xb).squeeze(1)\n", "                loss = loss_fn(pred, yb)\n", "                loss.backward(); opt.step()\n", "        losses.append(loss.item())\n", "        if verbose and (ep+1) % max(1, epochs//10) == 0:\n", "            print(f\"epoch {ep+1:4d}  mse={losses[-1]:.4f}\")\n", "    h = History(); h.epoch = list(range(epochs)); h.history = {\"loss\": losses}\n", "    return h"]}, {"cell_type": "markdown", "metadata": {"id": "9PsvDVEjVE7N"}, "source": ["## Let's first generate some data..."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Generate simple linear-ish data\n", "x = np.random.uniform(-1, 1, 100)\n", "y = 0.1 * x + np.random.normal(0, 0.025, 100)\n", "plt.figure(figsize=(4,3)); plt.scatter(x, y, label=\"data\"); plt.legend(); plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "2Lxez5J_VE7O"}, "source": ["## The standard way to deal with this, is through linear regression"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Linear regression via numpy (degree 1 polyfit) + visualization\n", "res = np.polyfit(x, y, 1)\n", "print(\"polyfit degree-1 coefficients [slope, intercept]:\", res)\n", "\n", "x_plot = np.linspace(-1, 1, 400)\n", "y_poly = np.polyval(res, x_plot)\n", "\n", "plt.figure(figsize=(4,3))\n", "plt.scatter(x, y, label=\"data\", s=20)\n", "plt.plot(x_plot, y_poly, label=\"polyfit deg=1\", color=\"C1\")\n", "plt.legend(); plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "GiSadI8eVE7O"}, "source": ["## Now, let's try to write the linear regression in a different way (more complicated way)"]}, {"cell_type": "markdown", "metadata": {"id": "ESychuhYVE7P"}, "source": ["The Dense command here, onnly says that the input is multiplied by a parameter $w$. We are effectively writing a simple model for our data: $y = w.a+b$, where $w$ is unknown.\n", "![alt](https://drive.google.com/uc?id=1Rt2bNPCxaHXdjzmVS7TCw_u_Ur-WIqlW)"]}, {"cell_type": "markdown", "metadata": {"id": "UmG2cEfWVE7P"}, "source": ["We can visualize the model we just created."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Define a *linear* neural net: y = W x + b\n", "ann = nn.Linear(1, 1).to(device)\n", "print(ann)"]}, {"cell_type": "markdown", "metadata": {"id": "Ts2m-GUXVE7Q"}, "source": ["### We then compile"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# In PyTorch, \"compile\" = choose loss + optimizer (no separate compile() step)\n", "criterion = nn.MSELoss()\n", "optimizer = torch.optim.Adam(ann.parameters(), lr=1e-2)\n", "print(\"Criterion:\", criterion, \"| Optimizer:\", optimizer)"]}, {"cell_type": "markdown", "metadata": {"id": "5Ax3CgZFVE7Q"}, "source": ["We are simply tht we want to minimize the mean square error (mse) between input and output. We call this the \"loss function\". So we are looking for the value of $w$ that minimizes the following expression: $$\\sum_i(y_i-w.x_i)^2$$"]}, {"cell_type": "markdown", "metadata": {"id": "sc9SHME0VE7Q"}, "source": ["### And fit the model ..."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Train the linear model\n", "history = train_regressor(ann, x, y, epochs=200, lr=1e-2, bs=None, verbose=False)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Plot training loss (Keras-like history object)\n", "plt.figure(figsize=(4,3))\n", "plt.plot(history.epoch, history.history[\"loss\"])\n", "plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.title(\"Training loss (linear model)\"); plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "JImVcZLyVE7R"}, "source": ["### Let's see what we got here..."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Show learned line vs data & polyfit\n", "y_ann = predict(ann, x_plot)\n", "\n", "plt.figure(figsize=(4,3))\n", "plt.scatter(x, y, label=\"data\", s=20)\n", "plt.plot(x_plot, y_ann, color=\"red\", label=\"ANN\")\n", "plt.plot(x_plot, np.polyval(res, x_plot), label=\"polyfit deg=1\", color=\"C1\")\n", "plt.legend(); plt.show()\n", "\n", "# Parity plot (model vs ANN) inside the training range\n", "model_true = lambda u: 0.1 * u\n", "y_true = model_true(x_plot)\n", "plt.figure(figsize=(4,3))\n", "plt.scatter(y_true, y_ann, s=10)\n", "m = [y_true.min(), y_true.max()]\n", "plt.plot(m, m, color=\"red\"); plt.gca().set_aspect('equal', adjustable='box')\n", "plt.xlabel(\"true y\"); plt.ylabel(\"ann y\"); plt.title(\"Parity (linear data)\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "IncN226-VE7R"}, "source": ["We have performed a linear regression with and artifical neural network ! So, yes, linear regression IS also Machine Learning..."]}, {"cell_type": "markdown", "metadata": {"id": "8zpwhMgfVE7S"}, "source": ["### Why is this useful ?"]}, {"cell_type": "markdown", "metadata": {"id": "u6y3vMj7VE7S"}, "source": ["Let's suppose we have a more complex dataset..."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Generate a more complex dataset and visualize it\n", "x = np.random.uniform(-1, 1, 100)\n", "model_true = lambda u: 0.1 * u + np.sin(5 * u)\n", "y = model_true(x) + np.random.normal(0, 0.45 * np.abs(x), 100)\n", "\n", "plt.figure(figsize=(4,3))\n", "plt.scatter(x, y, label=\"data\")\n", "plt.legend(); plt.show()\n", "\n", "x_plot = np.linspace(-1, 1, 400)"]}, {"cell_type": "markdown", "metadata": {"id": "SWeuuDrAVE7S"}, "source": ["## We can try again simple polynomial regression ..."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Polynomial regression baselines (deg=1,3,5)\n", "poly = [(deg, np.polyfit(x, y, deg)) for deg in [1, 3, 5]]\n", "\n", "plt.figure(figsize=(5,4))\n", "plt.scatter(x, y, label=\"data\", s=15)\n", "for deg, coeff in poly:\n", "    plt.plot(x_plot, np.polyval(coeff, x_plot), label=f\"polyfit deg={deg}\")\n", "plt.legend(); plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "chYup1A3VE7T"}, "source": ["but that will not work super well as expected..."]}, {"cell_type": "markdown", "metadata": {"id": "WIFjpJT1VE7T"}, "source": ["# Let's go back to our network..."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Linear ANN (no hidden layer) on complex data\n", "ann_lin = nn.Linear(1, 1).to(device)\n", "hist_lin = train_regressor(ann_lin, x, y, epochs=400, lr=1e-2, bs=None, verbose=False)"]}, {"cell_type": "markdown", "metadata": {"id": "XwCzDHC6VE7U"}, "source": ["If I do not change anything, I will obtain the same result. My model is simply linear..."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Visualize linear ANN predictions\n", "y_lin = predict(ann_lin, x_plot)\n", "plt.figure(figsize=(5,4))\n", "plt.scatter(x, y, label=\"data\", s=15)\n", "plt.plot(x_plot, y_lin, color=\"red\", label=\"ANN linear\")\n", "plt.legend(); plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "WLosTF5eVE7U"}, "source": ["## and add a bit of non-linearity ..."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Add a bit of non-linearity: 1 hidden layer\n", "ann_shallow = nn.Sequential(\n", "    nn.Linear(1, 16), nn.Tanh(),\n", "    nn.Linear(16, 1),\n", ").to(device)\n", "\n", "hist_shallow = train_regressor(ann_shallow, x, y, epochs=600, lr=1e-2, bs=32, verbose=False)\n", "\n", "# Loss curve\n", "plt.figure(figsize=(4,3))\n", "plt.plot(hist_shallow.epoch, hist_shallow.history[\"loss\"])\n", "plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.title(\"Training loss (1 hidden layer)\"); plt.show()\n", "\n", "# Predictions vs data and polyfits\n", "y_shallow = predict(ann_shallow, x_plot)\n", "plt.figure(figsize=(6,4))\n", "plt.scatter(x, y, label=\"data\", s=12)\n", "for deg, coeff in poly:\n", "    plt.plot(x_plot, np.polyval(coeff, x_plot), label=f\"polyfit deg={deg}\")\n", "plt.plot(x_plot, y_shallow, color=\"red\", label=\"ANN (1 hidden)\")\n", "plt.plot(x_plot, model_true(x_plot), color=\"black\", label=\"true model\")\n", "plt.legend(); plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "gLD4zYR7VE7V"}, "source": ["The sigmoid function is given by this expression: $$ \\frac{1}{1+e^{-x}}$$\n", "So our model is now like this:![alt](https://drive.google.com/uc?id=1-2VbatzRnqGJMKCga-tppiTo6iPRBr9s)\n", "This is what we call a perceptron. The non-linear function added after the linear combination is also called the activation function, because \"it fires the unit\"."]}, {"cell_type": "markdown", "metadata": {"id": "PK_EjsP9VE7W"}, "source": ["Still not great, but there is some potential !?"]}, {"cell_type": "markdown", "metadata": {"id": "kSWG7_EvVE7X"}, "source": ["## We can add another layer"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Add another layer (slightly deeper)\n", "ann_two = nn.Sequential(\n", "    nn.Linear(1, 32), nn.ReLU(),\n", "    nn.Linear(32, 16), nn.ReLU(),\n", "    nn.Linear(16, 1),\n", ").to(device)\n", "\n", "hist_two = train_regressor(ann_two, x, y, epochs=800, lr=1e-2, bs=32, verbose=False)\n", "\n", "# Quick look at predictions\n", "y_two = predict(ann_two, x_plot)\n", "plt.figure(figsize=(6,4))\n", "plt.scatter(x, y, label=\"data\", s=12)\n", "plt.plot(x_plot, y_two, color=\"red\", label=\"ANN (2 hidden)\")\n", "plt.plot(x_plot, model_true(x_plot), color=\"black\", label=\"true model\")\n", "plt.legend(); plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "yYiXwT_rVE7X"}, "source": ["We have added \"a layer\". Our model is now: $$ y=(\\frac{1}{1+e^{-(w_1.x)}}).w_2$$\n", "![alt](https://drive.google.com/uc?id=1E0iobni7jhUI2jfGKPb081OM_QDB5Hjg)"]}, {"cell_type": "markdown", "metadata": {"id": "SUEkYCI5VE7Y"}, "source": ["Not fantastic, but you get the idea...You have just created your first ANN for regression!"]}, {"cell_type": "markdown", "metadata": {"id": "priNDi2BVE7Y"}, "source": ["In fact, it turns out that it exists a mathematical theorem that proves that NNs are optimal approximators:\n"]}, {"cell_type": "markdown", "metadata": {"id": "ACHz-ClLVE7Y"}, "source": ["> _For any continuous function for a hypercube [0,1]d to real numbers, and every positive epsilon, there exists a sigmoid based 1-hidden layer neural network that obtaines at most epsilon error in functional space_  \n", "> Cybenko+89"]}, {"cell_type": "markdown", "metadata": {"id": "VL3KMJM2VE7Z"}, "source": ["> _Big enough network can approximate, but not represent any smooth function. the math demonstration implies showing that networs are dense in the space of target functions_"]}, {"cell_type": "markdown", "metadata": {"id": "ykn8zyUfVE7Z"}, "source": ["So, the approximation theorem tells me that there exists a NN that can approximate any function. It does not tell me which one: this is the alchemia of ML. It does not tell me how to minimize it either!"]}, {"cell_type": "markdown", "metadata": {"id": "FHkTxckiVE7Z"}, "source": ["## Let's make the model more complex"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Make the model/data more complex\n", "x = np.random.uniform(-1, 1, 100)\n", "model_true = lambda u: 0.1 * u + np.sin(5 * u)\n", "y = model_true(x) + np.random.normal(0, 0.45 * np.abs(x), 100)\n", "\n", "plt.figure(figsize=(4,3))\n", "plt.scatter(x, y, label=\"data\"); plt.legend(); plt.show()\n", "\n", "# Polynomial baselines\n", "poly = [(deg, np.polyfit(x, y, deg)) for deg in [1, 3, 5]]\n", "x_plot = np.linspace(-1, 1, 400)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# A slightly deeper neural net to capture nonlinearity\n", "ann = nn.Sequential(\n", "    nn.Linear(1, 10), nn.ReLU(),\n", "    nn.Linear(10, 5), nn.ReLU(),\n", "    nn.Linear(5, 1),\n", ").to(device)\n", "\n", "history = train_regressor(ann, x, y, epochs=800, lr=1e-2, bs=16, verbose=False)\n", "\n", "# Loss curve\n", "plt.figure(figsize=(4,3))\n", "plt.plot(history.epoch, history.history[\"loss\"]); plt.xlabel(\"epoch\"); plt.ylabel(\"loss\")\n", "plt.title(\"Training loss (nonlinear model)\"); plt.show()\n", "\n", "# Predictions vs baselines\n", "y_ann = predict(ann, x_plot)\n", "\n", "plt.figure(figsize=(5,4))\n", "plt.scatter(x, y, label=\"data\", s=15)\n", "for deg, coeff in poly:\n", "    plt.plot(x_plot, np.polyval(coeff, x_plot), label=f\"polyfit deg={deg}\")\n", "plt.plot(x_plot, y_ann, color=\"red\", label=\"ANN\")\n", "plt.plot(x_plot, model_true(x_plot), color=\"black\", label=\"true model\")\n", "plt.legend(); plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "s766SrdWVE7a"}, "source": ["Which is not that far from the real underlying model..."]}, {"cell_type": "markdown", "metadata": {"id": "oUWxnI867MUh"}, "source": ["### What if I go beyond my training set?"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Extrapolation beyond training range\n", "x_plot_large = np.linspace(-2, 2, 400)\n", "y_ann_large = predict(ann, x_plot_large)\n", "plt.figure(figsize=(5,4))\n", "plt.plot(x_plot_large, y_ann_large, color=\"red\", label=\"ANN\")\n", "plt.plot(x_plot_large, model_true(x_plot_large), label=\"model\", color=\"black\")\n", "plt.scatter(x, y, s=15, label=\"data\"); plt.legend(); plt.show()\n", "\n", "# Parity (wide range)\n", "y_predict = y_ann_large\n", "plt.figure(figsize=(4,3))\n", "plt.scatter(model_true(x_plot_large), y_predict, s=8)\n", "plt.plot([-2, 2], [-2, 2], color=\"red\")\n", "plt.gca().set_aspect('equal', adjustable='box')\n", "plt.xlabel(\"true y\"); plt.ylabel(\"ann y\"); plt.title(\"Parity (extrapolation)\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "6o9gDwfOVE7a"}, "source": ["## What about errors? Can we capture the uncertainties in the data?"]}], "metadata": {"accelerator": "GPU", "anaconda-cloud": {}, "celltoolbar": "Slideshow", "colab": {"gpuType": "T4", "provenance": []}, "kernelspec": {"display_name": "Python 3", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.4"}}, "nbformat": 4, "nbformat_minor": 0}