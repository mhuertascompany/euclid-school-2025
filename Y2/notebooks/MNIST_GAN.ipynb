{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# A Minimal GAN on MNIST \u2014 From Scratch in PyTorch\n", "\n", "This notebook trains a **basic GAN** on MNIST with the discriminator acting as a **binary classifier**:\n", "- **Generator** \\(G_\\theta(z)\\) maps noise \\(z\\sim\\mathcal N(0,I)\\) to images.\n", "- **Discriminator** \\(D_\\psi(x)\\in(0,1)\\) predicts the probability that an image is **real**.\n", "- **Losses (non\u2011saturating)**  \n", "  \\(\\displaystyle \\mathcal L_D = \\mathrm{BCE}(D(x_{\\text{real}}),1)+\\mathrm{BCE}(D(G(z)),0)\\)  \n", "  \\(\\displaystyle \\mathcal L_G = \\mathrm{BCE}(D(G(z)),1)\\)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1) Setup"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import os, math, time\n", "import numpy as np\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import torch.optim as optim\n", "from torch.utils.data import DataLoader\n", "import torchvision\n", "from torchvision import transforms, datasets, utils as vutils\n", "import matplotlib.pyplot as plt\n", "\n", "torch.manual_seed(0); np.random.seed(0)\n", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "print(\"device:\", device)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2) Hyperparameters"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["img_size   = 28\n", "img_dim    = 1 * img_size * img_size\n", "z_dim      = 100\n", "g_widths   = [256, 512, 1024]\n", "d_widths   = [512, 256]\n", "\n", "batch_size = 128\n", "epochs     = 30\n", "lr         = 2e-4\n", "betas      = (0.5, 0.999)\n", "label_smooth_real = 0.9\n", "\n", "sample_every = 1\n", "fixed_z = torch.randn(64, z_dim, device=device)\n", "out_dir = \"gan_mnist_outputs\"\n", "os.makedirs(out_dir, exist_ok=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3) Data loading (MNIST)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["transform = transforms.Compose([\n", "    transforms.ToTensor(),\n", "    transforms.Normalize((0.5,), (0.5,))\n", "])\n", "train_data = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n", "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n", "\n", "real_batch = next(iter(train_loader))[0][:64]\n", "grid = vutils.make_grid(real_batch, nrow=8, normalize=True, value_range=(-1,1))\n", "plt.figure(figsize=(6,6)); plt.axis(\"off\"); plt.title(\"Real MNIST samples (normalized)\")\n", "plt.imshow(np.transpose(grid.cpu().numpy(), (1,2,0)))\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4) Models \u2014 simple MLP GAN"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["class Generator(nn.Module):\n", "    def __init__(self, z_dim=100, widths=[256,512,1024], out_dim=784):\n", "        super().__init__()\n", "        layers = []\n", "        in_f = z_dim\n", "        for w in widths:\n", "            layers += [nn.Linear(in_f, w), nn.BatchNorm1d(w), nn.ReLU(True)]\n", "            in_f = w\n", "        layers += [nn.Linear(in_f, out_dim), nn.Tanh()]\n", "        self.net = nn.Sequential(*layers)\n", "    def forward(self, z):\n", "        x = self.net(z).view(-1, 1, 28, 28)\n", "        return x\n", "\n", "class Discriminator(nn.Module):\n", "    def __init__(self, in_dim=784, widths=[512,256]):\n", "        super().__init__()\n", "        layers = []\n", "        in_f = in_dim\n", "        for w in widths:\n", "            layers += [nn.Linear(in_f, w), nn.LeakyReLU(0.2, inplace=True)]\n", "            in_f = w\n", "        layers += [nn.Linear(in_f, 1)]\n", "        self.net = nn.Sequential(*layers)\n", "    def forward(self, x):\n", "        x = x.view(x.size(0), -1)\n", "        return self.net(x).squeeze(-1)\n", "\n", "G = Generator(z_dim, g_widths, img_dim).to(device)\n", "D = Discriminator(img_dim, d_widths).to(device)\n", "\n", "print(G); print(D)\n", "print(\"Params G:\", sum(p.numel() for p in G.parameters() if p.requires_grad))\n", "print(\"Params D:\", sum(p.numel() for p in D.parameters() if p.requires_grad))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5) Losses & optimizers (non\u2011saturating GAN)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["bce = nn.BCEWithLogitsLoss()\n", "optG = optim.Adam(G.parameters(), lr=lr, betas=betas)\n", "optD = optim.Adam(D.parameters(), lr=lr, betas=betas)\n", "\n", "def d_loss(real_logits, fake_logits, smooth=0.0):\n", "    targets_real = torch.full_like(real_logits, 1.0 - smooth)\n", "    targets_fake = torch.zeros_like(fake_logits)\n", "    return bce(real_logits, targets_real) + bce(fake_logits, targets_fake)\n", "\n", "def g_loss(fake_logits):\n", "    targets = torch.ones_like(fake_logits)\n", "    return bce(fake_logits, targets)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6) Training loop"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["log = {\"D\": [], \"G\": []}; step = 0\n", "\n", "for ep in range(1, epochs+1):\n", "    t0 = time.time()\n", "    for real, _ in train_loader:\n", "        real = real.to(device, non_blocking=True)\n", "        B = real.size(0)\n", "\n", "        # Update D\n", "        z = torch.randn(B, z_dim, device=device)\n", "        fake = G(z).detach()\n", "        logits_real = D(real)\n", "        logits_fake = D(fake)\n", "        loss_D = d_loss(logits_real, logits_fake, smooth=1.0 - label_smooth_real)\n", "        optD.zero_grad(); loss_D.backward(); optD.step()\n", "\n", "        # Update G\n", "        z = torch.randn(B, z_dim, device=device)\n", "        fake = G(z)\n", "        logits_fake = D(fake)\n", "        loss_G = g_loss(logits_fake)\n", "        optG.zero_grad(); loss_G.backward(); optG.step()\n", "\n", "        log[\"D\"].append(loss_D.item()); log[\"G\"].append(loss_G.item())\n", "        step += 1\n", "\n", "    # Sampling\n", "    if ep % sample_every == 0:\n", "        with torch.no_grad():\n", "            fake = G(fixed_z).cpu()\n", "            grid = vutils.make_grid(fake, nrow=8, normalize=True, value_range=(-1,1))\n", "        plt.figure(figsize=(6,6)); plt.axis(\"off\")\n", "        plt.title(f\"Generated samples @ epoch {ep}\")\n", "        plt.imshow(np.transpose(grid.numpy(), (1,2,0))); plt.show()\n", "        vutils.save_image(fake, os.path.join(out_dir, f\"samples_epoch_{ep:03d}.png\"),\n", "                          nrow=8, normalize=True, value_range=(-1,1))\n", "    print(f\"[{ep:02d}/{epochs}] D={np.mean(log['D'][-len(train_loader):]):.3f} \"\n", "          f\"G={np.mean(log['G'][-len(train_loader):]):.3f}  ({time.time()-t0:.1f}s)\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7) Loss curves"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["plt.figure(figsize=(6,3))\n", "plt.plot(log[\"D\"], label=\"D loss\")\n", "plt.plot(log[\"G\"], label=\"G loss\")\n", "plt.xlabel(\"training step\"); plt.ylabel(\"loss\")\n", "plt.legend(); plt.tight_layout(); plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8) Final sampling"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["with torch.no_grad():\n", "    z = torch.randn(64, z_dim, device=device)\n", "    fake = G(z).cpu()\n", "grid = vutils.make_grid(fake, nrow=8, normalize=True, value_range=(-1,1))\n", "plt.figure(figsize=(6,6)); plt.axis(\"off\")\n", "plt.title(\"Final generated samples\")\n", "plt.imshow(np.transpose(grid.numpy(), (1,2,0))); plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 9) (Optional) Minimax generator loss\n", "\n", "To try the original **minimax** generator loss (more prone to saturation), replace `g_loss` with:\n", "\n", "```python\n", "def g_loss_minimax(fake_logits):\n", "    # minimize E[ log(1 - D(G(z))) ] using BCE with target 0\n", "    targets = torch.zeros_like(fake_logits)\n", "    return bce(fake_logits, targets)\n", "```"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}