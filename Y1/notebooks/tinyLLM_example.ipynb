{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TinyLLM: A Small Transformer for Text Generation \n",
    "## Rodolphe Cledassou School 2025\n",
    "\n",
    "> Marc Huertas-Company & Hubert BretonniÃ¨re\n",
    "\n",
    "This notebook trains a **tiny Transformer decoder** (a minimal LLM) for **character-level** text generation using a small dataset from Hugging Face. It is designed to be **simple, fast, and pedagogical**, so it can run on a laptop (CPU or Mac MPS).\n",
    "\n",
    "What you'll see:\n",
    "- Loading a tiny dataset (**Tiny Shakespeare**).\n",
    "- Building a **char-level tokenizer** from scratch.\n",
    "- Implementing a **Transformer decoder** (multi-head self-attention + MLP).\n",
    "- Training with cross-entropy next-token prediction.\n",
    "- **Sampling** with temperature and top-*k*.\n",
    "- **Attention map** visualizations for interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Setup (device & optional installs)\n",
    "\n",
    "If you don't have the `datasets` library, uncomment the first cell to install."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What this cell does (Setup & Device):**\n",
    "- Imports core libraries (`torch`, `datasets`) and picks a compute **device**:\n",
    "  - Prefer **MPS** (Apple Metal) on Macs, then **CUDA**, else **CPU**.\n",
    "- Sets **random seeds** for reproducibility.\n",
    "- If `datasets` is missing, you can `pip install datasets` (comment in the cell).\n",
    "**Key ideas:** you must move both **model** and **batches** to the same device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d3d2fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marchuertascompany/soft/miniforge3/envs/spender/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# If needed, uncomment:\n",
    "# %pip install -q datasets\n",
    "\n",
    "import math, time, random, os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "torch.manual_seed(42); np.random.seed(42); random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2056f6",
   "metadata": {},
   "source": [
    "## 1) Load a tiny dataset (Tiny Shakespeare)\n",
    "\n",
    "We use the **`tiny_shakespeare`** dataset (about 1 MB). It's small but has rich structure to learn character-level language modeling.\n",
    "\n",
    "- We concatenate the text into one long string.\n",
    "- Then we will split into **train/val** segments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817a818f",
   "metadata": {},
   "source": [
    "**What this cell does (Data loading):**\n",
    "- Downloads the **Tiny Shakespeare** corpus from ðŸ¤— Datasets.\n",
    "- Concatenates all text chunks into a single string `raw_text` for easy indexing.\n",
    "**Why:** character-level LM treats the entire corpus as one long stream of tokens.\n",
    "**Tip:** If `tiny_shakespeare` fails on your setup, switch to the generic text loader with a raw URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24325c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "try:\n",
    "    ds = load_dataset(\"winglian/tiny-shakespeare\")\n",
    "    raw_text = \"\\n\".join(ds[\"train\"][\"text\"])\n",
    "except Exception:\n",
    "    ds = load_dataset(\n",
    "        \"text\",\n",
    "        data_files={\"train\": \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"},\n",
    "    )\n",
    "    raw_text = \"\\n\".join(ds[\"train\"][\"text\"])\n",
    "print(raw_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8589bd",
   "metadata": {},
   "source": [
    "## 2) Character vocabulary & encoding/decoding\n",
    "\n",
    "For pedagogy, we do **character-level** modeling:\n",
    "- Build `vocab` as the sorted unique characters in the dataset.\n",
    "- Create `stoi` / `itos` maps to encode/decode between text and integer IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db898ba2",
   "metadata": {},
   "source": [
    "**What this cell does (Character vocabulary):**\n",
    "- Builds a **character-level vocabulary** from unique chars in the corpus.\n",
    "- Creates maps:\n",
    "  - `stoi` (stringâ†’id) to **encode** text to integers.\n",
    "  - `itos` (idâ†’string) to **decode** integers back to text.\n",
    "**Why:** this avoids an external tokenizer and keeps the tutorial minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21dd52b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 65\n",
      "First 100 chars of vocab: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(raw_text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"First 100 chars of vocab:\", \"\".join(chars[:100]))\n",
    "\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for ch,i in stoi.items()}\n",
    "\n",
    "def encode(s: str):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(ids):\n",
    "    return \"\".join(itos[i] for i in ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ed4961",
   "metadata": {},
   "source": [
    "## 3) Train/Val split and batching\n",
    "\n",
    "- Convert the entire text into a **1D tensor of token IDs**.\n",
    "- Split into **90% train** and **10% val**.\n",
    "- Define a `get_batch(split)` function that samples random contiguous blocks of length **`block_size`**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a93c01f",
   "metadata": {},
   "source": [
    "**What this cell does (Train/Val split & batching):**\n",
    "- Converts the entire text into a 1D tensor of token IDs.\n",
    "- Splits into **90% train** / **10% validation**.\n",
    "- Defines `block_size` (context window) and `batch_size`.\n",
    "- Implements `get_batch(split)` that samples random **contiguous** windows:\n",
    "  - `x` is tokens `[t, t+1, ..., t+block_size-1]`\n",
    "  - `y` is next tokens `[t+1, ..., t+block_size]`\n",
    "**Why:** next-token prediction trains the model to predict the next character given the previous **context**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba06e42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tokens: 1,100,542 | Val tokens: 122,283\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(raw_text), dtype=torch.long)\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train_ids = data[:n]\n",
    "val_ids   = data[n:]\n",
    "\n",
    "block_size = 128\n",
    "batch_size = 64\n",
    "print(f\"Train tokens: {train_ids.numel():,} | Val tokens: {val_ids.numel():,}\")\n",
    "\n",
    "def get_batch(split):\n",
    "    ids = train_ids if split == \"train\" else val_ids\n",
    "    ix = torch.randint(0, len(ids) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([ids[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([ids[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80def297",
   "metadata": {},
   "source": [
    "## 4) Tiny Transformer decoder (from scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b4af36",
   "metadata": {},
   "source": [
    "**What this cell does (Tiny Transformer model):**\n",
    "- Implements a **GPT-style** decoder block from scratch:\n",
    "  - **Token & positional embeddings** (learned).\n",
    "  - **Causal multi-head self-attention** with a **lower-triangular mask** so tokens cannot see the future.\n",
    "  - **Pre-LayerNorm** + **MLP** with GELU and residual connections.\n",
    "- The attention step computes `softmax(QKáµ€/âˆšd)Â·V`, then projects back to the model dimension.\n",
    "**Outputs:** logits over the vocabulary for each position; if `targets` given, returns **cross-entropy loss**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52733099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, dropout, block_size):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = n_embd // n_head\n",
    "\n",
    "        self.q_proj = nn.Linear(n_embd, n_embd)\n",
    "        self.k_proj = nn.Linear(n_embd, n_embd)\n",
    "        self.v_proj = nn.Linear(n_embd, n_embd)\n",
    "        self.out_proj = nn.Linear(n_embd, n_embd)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.resid_drop = nn.Dropout(dropout)\n",
    "\n",
    "        mask = torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size)\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, x, return_attn=False):\n",
    "        B, T, C = x.shape\n",
    "        q = self.q_proj(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, -1)\n",
    "        y = self.resid_drop(self.out_proj(y))\n",
    "        if return_attn:\n",
    "            return y, att\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, dropout, block_size):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.sa = CausalSelfAttention(n_embd, n_head, dropout, block_size)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x, return_attn=False):\n",
    "        if return_attn:\n",
    "            y, att = self.sa(self.ln1(x), return_attn=True)\n",
    "            x = x + y\n",
    "            x = x + self.mlp(self.ln2(x))\n",
    "            return x, att\n",
    "        else:\n",
    "            x = x + self.sa(self.ln1(x))\n",
    "            x = x + self.mlp(self.ln2(x))\n",
    "            return x\n",
    "\n",
    "class TinyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd=128, n_head=4, n_layer=4, block_size=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_emb = nn.Embedding(block_size, n_embd)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList([Block(n_embd, n_head, dropout, block_size) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.head = nn.Linear(n_embd, vocab_size)\n",
    "        self.head.weight = self.tok_emb.weight  # weight tying\n",
    "\n",
    "    def forward(self, idx, targets=None, return_attn=False):\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.block_size\n",
    "        tok = self.tok_emb(idx)\n",
    "        pos = self.pos_emb(torch.arange(T, device=idx.device))\n",
    "        x = self.drop(tok + pos)\n",
    "\n",
    "        attn_maps = [] if return_attn else None\n",
    "        for blk in self.blocks:\n",
    "            if return_attn:\n",
    "                x, att = blk(x, return_attn=True)\n",
    "                attn_maps.append(att)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        if return_attn:\n",
    "            return logits, loss, attn_maps\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cb54ff",
   "metadata": {},
   "source": [
    "### Hyperparameters & model init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3973d90c",
   "metadata": {},
   "source": [
    "**What this cell does (Hyperparameters & initialization):**\n",
    "- Sets small, laptop-friendly sizes: embedding dim, heads, layers, dropout.\n",
    "- Instantiates the model and an **AdamW** optimizer.\n",
    "- Uses **weight tying** (output head shares weights with token embeddings) to reduce parameters and help training.\n",
    "**Rule of thumb:** larger `block_size` â‡’ longer context but more compute; start small and scale up gradually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35050195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters (M): 0.818113\n"
     ]
    }
   ],
   "source": [
    "n_embd = 128; n_head = 4; n_layer = 4; dropout = 0.1\n",
    "model = TinyTransformer(vocab_size, n_embd, n_head, n_layer, block_size, dropout).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\n",
    "print(\"Parameters (M):\", sum(p.numel() for p in model.parameters())/1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4904922",
   "metadata": {},
   "source": [
    "## 5) Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e579f432",
   "metadata": {},
   "source": [
    "**What this cell does (Training loop):**\n",
    "- Trains for a few hundred steps with **cross-entropy** next-token loss.\n",
    "- Every `eval_interval`, runs a quick **validation** estimate (no grad).\n",
    "- Uses **gradient clipping** to avoid exploding gradients.\n",
    "**Watch:** training loss should go down; validation loss may be a bit higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a88b7b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step   50 | train loss 2.555\n",
      "step  100 | train loss 2.553\n",
      "[eval] step 100 | train 2.431 | val 2.453\n",
      "step  150 | train loss 2.490\n",
      "step  200 | train loss 2.491\n",
      "[eval] step 200 | train 2.414 | val 2.438\n",
      "step  250 | train loss 2.512\n",
      "step  300 | train loss 2.465\n",
      "[eval] step 300 | train 2.375 | val 2.400\n",
      "step  350 | train loss 2.510\n",
      "step  400 | train loss 2.449\n",
      "[eval] step 400 | train 2.359 | val 2.390\n",
      "step  450 | train loss 2.438\n",
      "step  500 | train loss 2.409\n",
      "[eval] step 500 | train 2.294 | val 2.335\n",
      "step  550 | train loss 2.403\n",
      "step  600 | train loss 2.422\n",
      "[eval] step 600 | train 2.260 | val 2.298\n",
      "step  650 | train loss 2.395\n",
      "step  700 | train loss 2.343\n",
      "[eval] step 700 | train 2.244 | val 2.283\n",
      "step  750 | train loss 2.364\n",
      "step  800 | train loss 2.349\n",
      "[eval] step 800 | train 2.205 | val 2.239\n",
      "step  850 | train loss 2.334\n",
      "step  900 | train loss 2.285\n",
      "[eval] step 900 | train 2.176 | val 2.213\n",
      "step  950 | train loss 2.264\n",
      "step 1000 | train loss 2.249\n",
      "[eval] step 1000 | train 2.140 | val 2.185\n",
      "step 1050 | train loss 2.242\n",
      "step 1100 | train loss 2.246\n",
      "[eval] step 1100 | train 2.127 | val 2.165\n",
      "step 1150 | train loss 2.262\n",
      "step 1200 | train loss 2.236\n",
      "[eval] step 1200 | train 2.095 | val 2.144\n",
      "step 1250 | train loss 2.226\n",
      "step 1300 | train loss 2.217\n",
      "[eval] step 1300 | train 2.072 | val 2.130\n",
      "step 1350 | train loss 2.210\n",
      "step 1400 | train loss 2.188\n",
      "[eval] step 1400 | train 2.060 | val 2.114\n",
      "step 1450 | train loss 2.165\n",
      "step 1500 | train loss 2.159\n",
      "[eval] step 1500 | train 2.046 | val 2.099\n",
      "step 1550 | train loss 2.134\n",
      "step 1600 | train loss 2.140\n",
      "[eval] step 1600 | train 2.023 | val 2.079\n",
      "step 1650 | train loss 2.168\n",
      "step 1700 | train loss 2.123\n",
      "[eval] step 1700 | train 2.008 | val 2.070\n",
      "step 1750 | train loss 2.131\n",
      "step 1800 | train loss 2.102\n",
      "[eval] step 1800 | train 1.992 | val 2.064\n",
      "step 1850 | train loss 2.105\n",
      "step 1900 | train loss 2.124\n",
      "[eval] step 1900 | train 1.973 | val 2.033\n",
      "step 1950 | train loss 2.090\n",
      "step 2000 | train loss 2.093\n",
      "[eval] step 2000 | train 1.960 | val 2.032\n",
      "step 2050 | train loss 2.101\n",
      "step 2100 | train loss 2.031\n",
      "[eval] step 2100 | train 1.948 | val 2.025\n",
      "step 2150 | train loss 2.076\n",
      "step 2200 | train loss 2.073\n",
      "[eval] step 2200 | train 1.935 | val 2.006\n",
      "step 2250 | train loss 2.049\n",
      "step 2300 | train loss 2.051\n",
      "[eval] step 2300 | train 1.917 | val 1.998\n",
      "step 2350 | train loss 2.041\n",
      "step 2400 | train loss 2.036\n",
      "[eval] step 2400 | train 1.905 | val 1.978\n",
      "step 2450 | train loss 2.057\n",
      "step 2500 | train loss 2.047\n",
      "[eval] step 2500 | train 1.886 | val 1.965\n",
      "step 2550 | train loss 1.994\n",
      "step 2600 | train loss 2.014\n",
      "[eval] step 2600 | train 1.881 | val 1.958\n",
      "step 2650 | train loss 1.996\n",
      "step 2700 | train loss 1.976\n",
      "[eval] step 2700 | train 1.876 | val 1.957\n",
      "step 2750 | train loss 1.980\n",
      "step 2800 | train loss 1.983\n",
      "[eval] step 2800 | train 1.856 | val 1.951\n",
      "step 2850 | train loss 2.040\n",
      "step 2900 | train loss 1.958\n",
      "[eval] step 2900 | train 1.843 | val 1.936\n",
      "step 2950 | train loss 1.997\n",
      "step 3000 | train loss 1.980\n",
      "[eval] step 3000 | train 1.839 | val 1.933\n",
      "step 3050 | train loss 1.955\n",
      "step 3100 | train loss 1.984\n",
      "[eval] step 3100 | train 1.815 | val 1.921\n",
      "step 3150 | train loss 1.972\n",
      "step 3200 | train loss 1.933\n",
      "[eval] step 3200 | train 1.824 | val 1.912\n",
      "step 3250 | train loss 1.949\n",
      "step 3300 | train loss 1.967\n",
      "[eval] step 3300 | train 1.800 | val 1.904\n",
      "step 3350 | train loss 1.919\n",
      "step 3400 | train loss 1.923\n",
      "[eval] step 3400 | train 1.804 | val 1.894\n",
      "step 3450 | train loss 1.931\n",
      "step 3500 | train loss 1.935\n",
      "[eval] step 3500 | train 1.786 | val 1.899\n",
      "step 3550 | train loss 1.877\n",
      "step 3600 | train loss 1.917\n",
      "[eval] step 3600 | train 1.780 | val 1.892\n",
      "step 3650 | train loss 1.888\n",
      "step 3700 | train loss 1.878\n",
      "[eval] step 3700 | train 1.765 | val 1.883\n",
      "step 3750 | train loss 1.902\n",
      "step 3800 | train loss 1.861\n",
      "[eval] step 3800 | train 1.759 | val 1.879\n",
      "step 3850 | train loss 1.870\n",
      "step 3900 | train loss 1.880\n",
      "[eval] step 3900 | train 1.746 | val 1.862\n",
      "step 3950 | train loss 1.904\n",
      "step 4000 | train loss 1.832\n",
      "[eval] step 4000 | train 1.740 | val 1.864\n",
      "step 4050 | train loss 1.871\n",
      "step 4100 | train loss 1.868\n",
      "[eval] step 4100 | train 1.737 | val 1.855\n",
      "step 4150 | train loss 1.887\n",
      "step 4200 | train loss 1.865\n",
      "[eval] step 4200 | train 1.725 | val 1.850\n",
      "step 4250 | train loss 1.828\n",
      "step 4300 | train loss 1.840\n",
      "[eval] step 4300 | train 1.717 | val 1.841\n",
      "step 4350 | train loss 1.838\n",
      "step 4400 | train loss 1.845\n",
      "[eval] step 4400 | train 1.708 | val 1.828\n",
      "step 4450 | train loss 1.873\n",
      "step 4500 | train loss 1.836\n",
      "[eval] step 4500 | train 1.708 | val 1.823\n",
      "step 4550 | train loss 1.797\n",
      "step 4600 | train loss 1.826\n",
      "[eval] step 4600 | train 1.692 | val 1.823\n",
      "step 4650 | train loss 1.810\n",
      "step 4700 | train loss 1.818\n",
      "[eval] step 4700 | train 1.682 | val 1.821\n",
      "step 4750 | train loss 1.808\n",
      "step 4800 | train loss 1.845\n",
      "[eval] step 4800 | train 1.682 | val 1.813\n",
      "step 4850 | train loss 1.828\n",
      "step 4900 | train loss 1.829\n",
      "[eval] step 4900 | train 1.667 | val 1.802\n",
      "step 4950 | train loss 1.816\n",
      "step 5000 | train loss 1.789\n",
      "[eval] step 5000 | train 1.667 | val 1.789\n"
     ]
    }
   ],
   "source": [
    "max_steps = 5000\n",
    "eval_interval = 100\n",
    "log_every = 50\n",
    "\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    with torch.no_grad():\n",
    "        for split in [\"train\", \"val\"]:\n",
    "            losses = []\n",
    "            for _ in range(20):\n",
    "                xb, yb = get_batch(\"train\" if split==\"train\" else \"val\")\n",
    "                _, loss = model(xb, yb)\n",
    "                losses.append(loss.item())\n",
    "            out[split] = sum(losses)/len(losses)\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "model.train()\n",
    "for step in range(1, max_steps+1):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % log_every == 0:\n",
    "        print(f\"step {step:4d} | train loss {loss.item():.3f}\")\n",
    "    if step % eval_interval == 0 or step == max_steps:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"[eval] step {step} | train {losses['train']:.3f} | val {losses['val']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825b2ed8",
   "metadata": {},
   "source": [
    "## 6) Sampling (temperature & top-k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fca670",
   "metadata": {},
   "source": [
    "**What this cell does (Autoregressive sampling):**\n",
    "- Implements greedy/tempered **sampling**:\n",
    "  - Crops to the last `block_size` tokens (context window).\n",
    "  - Applies **temperature** (higher â‡’ more randomness).\n",
    "  - Optional **top-k** filtering keeps only the top-k logits (sharper outputs).\n",
    "- Generates new characters step-by-step appended to the prompt.\n",
    "**Tip:** try different temperatures (e.g., 0.7, 1.0, 1.3) and `top_k` (e.g., 20, 40) to see the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc68f595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SAMPLE ===\n",
      "ROMEO:\n",
      "And shall sir veity. \n",
      "Marmagainss:\n",
      "With and mors the the peace.\n",
      "\n",
      "CLARENCE:\n",
      "Nay, for but your orcarfew repicity.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "And like the love degriegedut; must, chince the\n",
      "That the comenty, besking in haved thim hous lace\n",
      "The blee his your retled unds nund my savend word\n",
      "The hand poin a your I\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, idx, max_new_tokens=200, temperature=1.0, top_k=None):\n",
    "    model.eval()\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -model.block_size:]\n",
    "        logits, _ = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / max(1e-8, temperature)\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float('inf')\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "    return idx\n",
    "\n",
    "prompt = \"ROMEO:\"\n",
    "prompt_ids = torch.tensor([ [stoi[c] for c in prompt] ], dtype=torch.long, device=device)\n",
    "gen_ids = generate(model, prompt_ids, max_new_tokens=300, temperature=0.9, top_k=40)\n",
    "print(\"=== SAMPLE ===\")\n",
    "print(\"\".join(itos[i] for i in gen_ids[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de86b2a",
   "metadata": {},
   "source": [
    "## 7) Attention maps (visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310bd873",
   "metadata": {},
   "source": [
    "**What this cell does (Attention visualization):**\n",
    "- Runs a short prompt through the model while collecting **attention weights**.\n",
    "- Plots one **head** of one **layer** as a heatmap:\n",
    "  - Rows = **query** positions (current token).\n",
    "  - Columns = **key** positions (context tokens).\n",
    "  - Brighter = higher attention weight.\n",
    "**Why:** attention maps give intuition about which characters/positions the model focuses on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2438e3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_text = \"ROMEO:\\n\"\n",
    "    idx = torch.tensor([[stoi[c] for c in sample_text]], dtype=torch.long, device=device)\n",
    "    idx = idx[:, :min(idx.size(1), model.block_size)]\n",
    "    logits, loss, attn_maps = model(idx, targets=None, return_attn=True)\n",
    "\n",
    "layer_to_show = 1\n",
    "head_to_show = 2\n",
    "att = attn_maps[layer_to_show][0, head_to_show].cpu().numpy()\n",
    "tokens = list(sample_text[:att.shape[0]])\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(att)\n",
    "plt.xticks(range(len(tokens)), tokens, rotation=90)\n",
    "plt.yticks(range(len(tokens)), tokens)\n",
    "plt.xlabel(\"Key positions\"); plt.ylabel(\"Query positions\")\n",
    "plt.title(f\"Attention â€” layer {layer_to_show}, head {head_to_show}\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Next steps\n",
    "- Try more steps or a slightly larger model.\n",
    "- Switch to **subword tokenization** (BPE) for more realistic language modeling.\n",
    "- Add **learning rate schedules**, **dropout tuning**, or **checkpointing**.\n",
    "- Visualize attention across different layers/heads and for different prompts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
