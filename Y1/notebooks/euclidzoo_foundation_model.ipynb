{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Tiny Transfer Learning with Self-Supervised Encoders (DINOv2 vs Zoobot)\n", "\n", "This notebook shows **pedagogical, laptop-friendly** transfer learning for image classification:\n", "1. **Self-supervised general encoder**: DINOv2-S (small) from Hugging Face \u2014 we freeze it and train a **linear probe** (logistic classifier) on your dataset.\n", "2. **Domain-specific encoder**: **Zoobot** (ConvNeXt Nano) pretrained on galaxy images \u2014 we repeat the same linear-probe protocol.\n", "3. **Compare** accuracy and confusion matrices.\n", "\n", "> Why linear probe? It\u2019s fast and isolates **feature quality** without conflating with heavy fine-tuning."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 0) Setup & installs\n", "\n", "- Installs: `transformers` (for DINOv2), `timm` (for Zoobot encoder), `datasets/torchvision` (data), `huggingface_hub` (model download).\n", "- Device: prefer **Apple Metal (MPS)** on Mac; else CUDA; else CPU."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# If needed, uncomment these installs (run once)\n", "# %pip install -q transformers timm datasets torchvision huggingface_hub Pillow\n", "\n", "import os, math, random, time, numpy as np\n", "import torch, torch.nn as nn, torch.nn.functional as F\n", "from torch.utils.data import DataLoader\n", "from torchvision import transforms, datasets\n", "\n", "from datasets import load_dataset\n", "from huggingface_hub import login\n", "\n", "device = (\n", "    torch.device(\"mps\") if torch.backends.mps.is_available()\n", "    else torch.device(\"cuda\") if torch.cuda.is_available()\n", "    else torch.device(\"cpu\")\n", ")\n", "print(\"Using device:\", device)\n", "\n", "torch.manual_seed(42); np.random.seed(42); random.seed(42)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1) Data: use your local folder if available, else fall back to a tiny HF dataset\n", "\n", "- If `./zoo-data/train/...` exists we load it as an ImageFolder.\n", "- Otherwise, we use the **Beans** dataset (3 classes, small) from \ud83e\udd17 Datasets to keep things light.\n", "- We apply standard **224\u00d7224** transforms (to match ViT/ConvNeXt defaults)."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["IMG_SIZE = 224\n", "BATCH_SIZE = 64\n", "\n", "from torchvision import transforms, datasets\n", "\n", "train_tfms = transforms.Compose([\n", "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n", "    transforms.ToTensor(),\n", "    transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n", "])\n", "test_tfms = transforms.Compose([\n", "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n", "    transforms.ToTensor(),\n", "    transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n", "])\n", "\n", "def dl_from_imagefolder(root=\"zoo-data\"):\n", "    train_dir, test_dir = os.path.join(root,\"train\"), os.path.join(root,\"test\")\n", "    if os.path.isdir(train_dir) and os.path.isdir(test_dir):\n", "        train_ds = datasets.ImageFolder(train_dir, transform=train_tfms)\n", "        test_ds  = datasets.ImageFolder(test_dir,  transform=test_tfms)\n", "        return train_ds, test_ds\n", "    return None, None\n", "\n", "train_ds, test_ds = dl_from_imagefolder(\"zoo-data\")\n", "\n", "if train_ds is None:\n", "    from datasets import load_dataset\n", "    hf = load_dataset(\"beans\")\n", "    class BeansTorch(torch.utils.data.Dataset):\n", "        def __init__(self, hf_split, tfm):\n", "            self.hf = hf_split\n", "            self.tfm = tfm\n", "            self.classes = [\"angular_leaf_spot\",\"bean_rust\",\"healthy\"]\n", "        def __len__(self): return len(self.hf)\n", "        def __getitem__(self, i):\n", "            ex = self.hf[i]\n", "            img = ex[\"image\"].convert(\"RGB\")\n", "            y = int(ex.get(\"labels\", ex.get(\"label\")))\n", "            return self.tfm(img), y\n", "    train_ds = BeansTorch(hf[\"train\"], train_tfms)\n", "    test_ds  = BeansTorch(hf[\"test\"],  test_tfms)\n", "\n", "NUM_CLASSES = len(getattr(train_ds, \"classes\", [])) or len(set([y for _,y in [train_ds[i] for i in range(min(200,len(train_ds)))] ]))\n", "print(\"Classes:\", getattr(train_ds,\"classes\", f\"{NUM_CLASSES} classes\"))\n", "\n", "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2)\n", "val_loader   = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2) Linear probe on **DINOv2-S** (frozen) from Hugging Face\n", "\n", "- Model: `facebook/dinov2-small`\n", "- We use the CLS token from the final hidden state as a **feature vector**.\n", "- We train a small **linear head** on top.\n", "- Then we evaluate validation accuracy."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["from transformers import AutoImageProcessor, AutoModel\n", "import numpy as np\n", "\n", "dinov2_id = \"facebook/dinov2-small\"\n", "processor = AutoImageProcessor.from_pretrained(dinov2_id)\n", "encoder = AutoModel.from_pretrained(dinov2_id).to(device)\n", "encoder.eval()\n", "for p in encoder.parameters(): p.requires_grad = False\n", "\n", "# infer hidden size\n", "with torch.no_grad():\n", "    dummy = torch.zeros(1, 3, IMG_SIZE, IMG_SIZE).to(device)\n", "    imgs = (dummy.permute(0,2,3,1).cpu().numpy()*255*0.5+127.5).clip(0,255).astype(np.uint8)\n", "    inputs = processor(images=list(imgs), return_tensors=\"pt\").to(device)\n", "    out = encoder(**inputs)\n", "    hidden_size = out.last_hidden_state.shape[-1]\n", "\n", "head = nn.Linear(hidden_size, NUM_CLASSES).to(device)\n", "optim = torch.optim.AdamW(head.parameters(), lr=1e-3, weight_decay=1e-2)\n", "criterion = nn.CrossEntropyLoss()\n", "\n", "def dinov2_features(batch_x):\n", "    imgs = (batch_x.permute(0,2,3,1).cpu().numpy()*255*0.5+127.5).clip(0,255).astype(np.uint8)\n", "    inputs = processor(images=list(imgs), return_tensors=\"pt\").to(device)\n", "    with torch.no_grad():\n", "        outputs = encoder(**inputs, output_attentions=False)\n", "        feats = outputs.last_hidden_state[:, 0, :]\n", "    return feats\n", "\n", "def train_linear_probe(head, train_loader, steps=300):\n", "    head.train()\n", "    it = iter(train_loader)\n", "    for step in range(1, steps+1):\n", "        try:\n", "            x, y = next(it)\n", "        except StopIteration:\n", "            it = iter(train_loader); x, y = next(it)\n", "        x, y = x.to(device), y.to(device)\n", "        feats = dinov2_features(x)\n", "        logits = head(feats)\n", "        loss = criterion(logits, y)\n", "        optim.zero_grad(set_to_none=True)\n", "        loss.backward(); optim.step()\n", "        if step % 50 == 0:\n", "            pred = logits.argmax(1)\n", "            acc = (pred==y).float().mean().item()\n", "            print(f\"[DINOv2] step {step:04d}  loss={loss.item():.4f}  acc(batch)={acc:.3f}\")\n", "\n", "@torch.no_grad()\n", "def eval_linear_probe(head, loader):\n", "    head.eval()\n", "    tot, correct, loss_sum = 0, 0, 0.0\n", "    for x,y in loader:\n", "        x,y = x.to(device), y.to(device)\n", "        feats = dinov2_features(x)\n", "        logits = head(feats)\n", "        loss = criterion(logits, y)\n", "        loss_sum += loss.item()*x.size(0)\n", "        pred = logits.argmax(1)\n", "        tot += y.size(0); correct += (pred==y).sum().item()\n", "    return loss_sum/tot, correct/tot\n", "\n", "print(\"Training linear probe (DINOv2-S, frozen encoder)...\")\n", "train_linear_probe(head, train_loader, steps=300)\n", "dino_val_loss, dino_val_acc = eval_linear_probe(head, val_loader)\n", "print(f\"[DINOv2] val loss={dino_val_loss:.4f}  acc={dino_val_acc:.4f}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.1) Attention map from DINOv2\n", "\n", "Visualize CLS attention to patches (last layer, head 0) for one validation example."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import matplotlib.pyplot as plt, math\n", "\n", "@torch.no_grad()\n", "def show_dinov2_attention(img_tensor):\n", "    img = (img_tensor.permute(1,2,0).cpu().numpy()*255*0.5+127.5).clip(0,255).astype(np.uint8)\n", "    inputs = processor(images=[img], return_tensors=\"pt\").to(device)\n", "    outputs = encoder(**inputs, output_attentions=True)\n", "    att = outputs.attentions[-1][0, 0]  # [seq, seq]\n", "    cls_to_patches = att[0].cpu().numpy()\n", "    L = int(math.sqrt(cls_to_patches.shape[0]-1)) if (cls_to_patches.shape[0]-1)>0 else 1\n", "    patch_map = cls_to_patches[1:].reshape(L, L)\n", "    plt.figure(figsize=(4,4)); plt.imshow(patch_map)\n", "    plt.title(\"DINOv2 \u2014 CLS attention to patches (last layer, head 0)\"); plt.axis(\"off\"); plt.colorbar(); plt.show()\n", "\n", "xb, yb = next(iter(val_loader))\n", "show_dinov2_attention(xb[0])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3) Linear probe on **Zoobot ConvNeXt Nano** (frozen)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import timm, torch\n", "\n", "zoobot_id = \"mwalmsley/zoobot-encoder-convnext_nano\"\n", "encoder_zoobot = timm.create_model(f\"hf_hub:{zoobot_id}\", pretrained=True, num_classes=0).to(device)\n", "encoder_zoobot.eval()\n", "for p in encoder_zoobot.parameters(): p.requires_grad = False\n", "\n", "with torch.no_grad():\n", "    dummy = torch.zeros(1, 3, IMG_SIZE, IMG_SIZE).to(device)\n", "    feats = encoder_zoobot(dummy)\n", "    feat_dim = feats.shape[-1] if feats.ndim == 2 else feats.flatten(1).shape[-1]\n", "\n", "head_z = nn.Linear(feat_dim, NUM_CLASSES).to(device)\n", "optim_z = torch.optim.AdamW(head_z.parameters(), lr=1e-3, weight_decay=1e-2)\n", "criterion = nn.CrossEntropyLoss()\n", "\n", "def zoobot_features(batch_x):\n", "    with torch.no_grad():\n", "        z = encoder_zoobot(batch_x)\n", "        if z.ndim > 2: z = z.flatten(1)\n", "    return z\n", "\n", "def train_linear_probe_zoobot(head, train_loader, steps=300):\n", "    head.train()\n", "    it = iter(train_loader)\n", "    for step in range(1, steps+1):\n", "        try:\n", "            x, y = next(it)\n", "        except StopIteration:\n", "            it = iter(train_loader); x, y = next(it)\n", "        x, y = x.to(device), y.to(device)\n", "        feats = zoobot_features(x)\n", "        logits = head(feats)\n", "        loss = criterion(logits, y)\n", "        optim_z.zero_grad(set_to_none=True)\n", "        loss.backward(); optim_z.step()\n", "        if step % 50 == 0:\n", "            pred = logits.argmax(1)\n", "            acc = (pred==y).float().mean().item()\n", "            print(f\"[Zoobot] step {step:04d}  loss={loss.item():.4f}  acc(batch)={acc:.3f}\")\n", "\n", "@torch.no_grad()\n", "def eval_linear_probe_zoobot(head, loader):\n", "    head.eval()\n", "    tot, correct, loss_sum = 0, 0, 0.0\n", "    for x,y in loader:\n", "        x,y = x.to(device), y.to(device)\n", "        feats = zoobot_features(x)\n", "        logits = head(feats)\n", "        loss = criterion(logits, y)\n", "        loss_sum += loss.item()*x.size(0)\n", "        pred = logits.argmax(1)\n", "        tot += y.size(0); correct += (pred==y).sum().item()\n", "    return loss_sum/tot, correct/tot\n", "\n", "print(\"Training linear probe (Zoobot ConvNeXt Nano, frozen encoder)...\")\n", "train_linear_probe_zoobot(head_z, train_loader, steps=300)\n", "zoobot_val_loss, zoobot_val_acc = eval_linear_probe_zoobot(head_z, val_loader)\n", "print(f\"[Zoobot] val loss={zoobot_val_loss:.4f}  acc={zoobot_val_acc:.4f}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4) Compare results + confusion matrices"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import numpy as np, matplotlib.pyplot as plt\n", "\n", "@torch.no_grad()\n", "def preds_for(head, loader, feats_fn):\n", "    y_true, y_pred = [], []\n", "    for x,y in loader:\n", "        x = x.to(device)\n", "        logits = head(feats_fn(x))\n", "        y_true.append(y.numpy())\n", "        y_pred.append(logits.argmax(1).cpu().numpy())\n", "    y_true = np.concatenate(y_true); y_pred = np.concatenate(y_pred)\n", "    return y_true, y_pred\n", "\n", "y_true_d, y_pred_d = preds_for(head,   val_loader, dinov2_features)\n", "y_true_z, y_pred_z = preds_for(head_z, val_loader, zoobot_features)\n", "\n", "labels = getattr(train_ds, \"classes\", [str(i) for i in range(int(max(y_true_d.max(), y_true_z.max()))+1)])\n", "def confusion_matrix_np(y_true, y_pred, K):\n", "    cm = np.zeros((K,K), dtype=int)\n", "    for t,p in zip(y_true, y_pred):\n", "        cm[int(t), int(p)] += 1\n", "    return cm\n", "\n", "cm_d = confusion_matrix_np(y_true_d, y_pred_d, len(labels))\n", "cm_z = confusion_matrix_np(y_true_z, y_pred_z, len(labels))\n", "\n", "def plot_cm(cm, labels, title, ax):\n", "    im = ax.imshow(cm, cmap=\"Blues\")\n", "    ax.set_title(title)\n", "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n", "    ax.set_xticks(range(len(labels))); ax.set_yticks(range(len(labels)))\n", "    ax.set_xticklabels(labels, rotation=45, ha=\"right\"); ax.set_yticklabels(labels)\n", "    for i in range(cm.shape[0]):\n", "        for j in range(cm.shape[1]):\n", "            ax.text(j, i, cm[i,j], ha=\"center\", va=\"center\", fontsize=8)\n", "    return im\n", "\n", "fig, axes = plt.subplots(1,2, figsize=(12,4))\n", "plot_cm(cm_d, labels, f\"DINOv2-S \u2014 acc={dino_val_acc:.3f}\", axes[0])\n", "im = plot_cm(cm_z, labels, f\"Zoobot ConvNeXt Nano \u2014 acc={zoobot_val_acc:.3f}\", axes[1])\n", "fig.colorbar(im, ax=axes.ravel().tolist(), fraction=0.046, pad=0.04)\n", "plt.tight_layout(); plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Notes & caveats\n", "- **Domain gap:** Zoobot is pretrained on **galaxy images**; if your dataset is not astronomy, it may underperform vs a general SSL model (DINOv2). On astronomy data, Zoobot should shine.\n", "- **Speed:** We froze encoders and trained only the linear head (300 steps). Increase steps or unfreeze the last block for higher accuracy (slower).\n", "- **Transforms:** Simple 224\u00d7224 + normalization; feel free to add augmentations.\n", "- **Attention maps:** DINOv2 (Transformers) exposes attentions; Zoobot via `timm` typically does not."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}