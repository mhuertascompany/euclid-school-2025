{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Two Moons with a Simple RealNVP (Normalizing Flow) \u2014 From Scratch in PyTorch\n", "\n", "This notebook builds a **RealNVP** flow from scratch to learn the **two-moon** density and\n", "plots the **intermediate transforms** from the base Gaussian to the target."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import math, os, time\n", "import numpy as np\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import matplotlib.pyplot as plt\n", "\n", "torch.manual_seed(0); np.random.seed(0)\n", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "print(\"device:\", device)\n", "\n", "# Hyperparameters\n", "n_samples    = 20000\n", "batch_size   = 512\n", "lr           = 2e-3\n", "epochs       = 40\n", "n_layers     = 6\n", "hidden_units = 128\n", "s_clip       = 3.0\n", "plot_every   = 10"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Two-moon target data"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def make_two_moons(n, noise=0.06, seed=0):\n", "    rng = np.random.default_rng(seed)\n", "    n1 = n//2; n2 = n - n1\n", "    t1 = rng.uniform(0, np.pi, size=n1)\n", "    x1 = np.stack([np.cos(t1), np.sin(t1)], axis=1)\n", "    t2 = rng.uniform(0, np.pi, size=n2)\n", "    x2 = np.stack([1.0 - np.cos(t2), -np.sin(t2) - 0.5], axis=1)\n", "    X = np.vstack([x1, x2]) + noise * rng.standard_normal(size=(n,2))\n", "    rng.shuffle(X)\n", "    return torch.tensor(X, dtype=torch.float32)\n", "\n", "X = make_two_moons(n_samples, noise=0.06, seed=42).to(device)\n", "print(\"Data shape:\", X.shape)\n", "\n", "plt.figure(figsize=(4,4))\n", "plt.scatter(X[:5000,0].cpu(), X[:5000,1].cpu(), s=4, alpha=0.7)\n", "plt.title(\"Two-moons target samples\"); plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.tight_layout(); plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## RealNVP building blocks\n", "\n", "Affine coupling layer with a binary mask `m` that keeps part of the input and\n", "affinely transforms the rest. Triangular Jacobian \u21d2 cheap log-determinant."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["class STNet(nn.Module):\n", "    # Small MLP to predict scale 's' and shift 't' from the masked inputs\n", "    def __init__(self, in_dim, out_dim, hidden=128):\n", "        super().__init__()\n", "        self.net = nn.Sequential(\n", "            nn.Linear(in_dim, hidden), nn.ReLU(),\n", "            nn.Linear(hidden, hidden), nn.ReLU(),\n", "            nn.Linear(hidden, 2*out_dim),\n", "        )\n", "    def forward(self, x):\n", "        st = self.net(x)\n", "        s, t = torch.chunk(st, 2, dim=-1)\n", "        return s, t\n", "\n", "class CouplingLayer(nn.Module):\n", "    def __init__(self, dim, mask, hidden=128, s_clip=3.0):\n", "        super().__init__()\n", "        self.register_buffer(\"mask\", mask)\n", "        in_dim  = int(mask.sum().item())\n", "        out_dim = dim - in_dim\n", "        self.st = STNet(in_dim, out_dim, hidden)\n", "        self.s_clip = s_clip\n", "\n", "    def forward(self, x):\n", "        # x -> z (used in log_prob)\n", "        x1 = x * self.mask\n", "        x2 = x * (1 - self.mask)\n", "        s, t = self.st(x1[:, self.mask.bool()])\n", "        if self.s_clip is not None:\n", "            s = torch.tanh(s) * self.s_clip\n", "        y2 = x2[:, (1 - self.mask).bool()] * torch.exp(s) + t\n", "        y = x.clone()\n", "        y[:, (1 - self.mask).bool()] = y2\n", "        logdet = s.sum(dim=-1)\n", "        return y, logdet\n", "\n", "    def inverse(self, y):\n", "        # y -> x (used in sampling)\n", "        y1 = y * self.mask\n", "        y2 = y * (1 - self.mask)\n", "        s, t = self.st(y1[:, self.mask.bool()])\n", "        if self.s_clip is not None:\n", "            s = torch.tanh(s) * self.s_clip\n", "        x2 = (y2[:, (1 - self.mask).bool()] - t) * torch.exp(-s)\n", "        x = y.clone()\n", "        x[:, (1 - self.mask).bool()] = x2\n", "        logdet = -s.sum(dim=-1)\n", "        return x, logdet\n", "\n", "class RealNVP(nn.Module):\n", "    def __init__(self, dim=2, n_layers=6, hidden=128, s_clip=3.0):\n", "        super().__init__()\n", "        self.dim = dim\n", "        masks = []\n", "        for i in range(n_layers):\n", "            m = torch.zeros(dim)\n", "            m[i % dim] = 1.0  # alternate which coord is passed-through\n", "            masks.append(m)\n", "        self.layers = nn.ModuleList([CouplingLayer(dim, m, hidden, s_clip) for m in masks])\n", "\n", "    def f(self, x):\n", "        # Forward transform: x -> z, accumulate log|det J|\n", "        logdet = torch.zeros(x.size(0), device=x.device)\n", "        z = x\n", "        for layer in self.layers:\n", "            z, ld = layer(z)\n", "            logdet = logdet + ld\n", "        return z, logdet\n", "\n", "    def f_inv(self, z):\n", "        # Inverse: z -> x\n", "        x = z\n", "        for layer in self.layers[::-1]:\n", "            x, _ = layer.inverse(x)\n", "        return x\n", "\n", "    def f_inv_prefix(self, z, k):\n", "        # Inverse through first k layers (for intermediate plots)\n", "        x = z\n", "        for layer in self.layers[:k][::-1]:\n", "            x, _ = layer.inverse(x)\n", "        return x\n", "\n", "    def log_prob(self, x):\n", "        z, logdet = self.f(x)\n", "        log_p0 = -0.5 * (z**2).sum(dim=1) - self.dim*0.5*math.log(2*math.pi)\n", "        return log_p0 + logdet\n", "\n", "    def sample(self, n):\n", "        z = torch.randn(n, self.dim, device=next(self.parameters()).device)\n", "        return self.f_inv(z)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["flow = RealNVP(dim=2, n_layers=n_layers, hidden=hidden_units, s_clip=s_clip).to(device)\n", "print(flow)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Plotting helpers (intermediate steps, density, etc.)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["@torch.no_grad()\n", "def plot_intermediate_flow(flow, n_points=5000):\n", "    z = torch.randn(n_points, 2, device=next(flow.parameters()).device)\n", "    L = len(flow.layers)\n", "    cols = min(4, L+1); rows = int(np.ceil((L+1)/cols))\n", "    plt.figure(figsize=(4*cols, 4*rows))\n", "    for k in range(L+1):\n", "        if k == 0:\n", "            xk = z.cpu(); title = \"Base (z)\"\n", "        else:\n", "            xk = flow.f_inv_prefix(z, k).cpu(); title = f\"After {k} layer(s)\"\n", "        ax = plt.subplot(rows, cols, k+1)\n", "        ax.scatter(xk[:,0], xk[:,1], s=4, alpha=0.5)\n", "        ax.set_title(title); ax.set_xlim(-3,3); ax.set_ylim(-3,3); ax.set_aspect('equal', adjustable='box')\n", "    plt.tight_layout(); plt.show()\n", "\n", "@torch.no_grad()\n", "def plot_data_and_density(flow, data, lim=3.0, grid_n=150):\n", "    xs = data[:5000].cpu().numpy()\n", "    grid = np.stack(np.meshgrid(np.linspace(-lim,lim,grid_n), np.linspace(-lim,lim,grid_n)), axis=-1).reshape(-1,2)\n", "    grid_t = torch.tensor(grid, dtype=torch.float32, device=next(flow.parameters()).device)\n", "    lp = flow.log_prob(grid_t).cpu().numpy().reshape(grid_n, grid_n)\n", "    Xg, Yg = np.linspace(-lim,lim,grid_n), np.linspace(-lim,lim,grid_n)\n", "    XG, YG = np.meshgrid(Xg, Yg)\n", "    plt.figure(figsize=(5,4))\n", "    plt.contourf(XG, YG, np.exp(lp), levels=40)\n", "    plt.colorbar(label=\"density\")\n", "    plt.scatter(xs[:,0], xs[:,1], s=5, c=\"k\", alpha=0.4)\n", "    plt.title(\"Data with learned density\")\n", "    plt.xlim(-3,3); plt.ylim(-3,3); plt.gca().set_aspect('equal', adjustable='box')\n", "    plt.tight_layout(); plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Train (maximum likelihood)\n", "Minimize negative log-likelihood:  \\(- \\frac{1}{B} \\sum_b \\log p_\\theta(x_b) \\)."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def train(flow, X, epochs=40, bs=512):\n", "    opt = torch.optim.Adam(flow.parameters(), lr=lr)\n", "    N = X.size(0)\n", "    for ep in range(1, epochs+1):\n", "        perm = torch.randperm(N, device=X.device)\n", "        total_nll = 0.0\n", "        for i in range(0, N, bs):\n", "            xb = X[perm[i:i+bs]]\n", "            loss = -flow.log_prob(xb).mean()\n", "            opt.zero_grad(); loss.backward(); opt.step()\n", "            total_nll += loss.item() * xb.size(0)\n", "        if ep % 5 == 0 or ep == 1:\n", "            print(f\"[{ep:3d}] NLL: {total_nll/N:.4f}\")\n", "        if ep % plot_every == 0 or ep == 1:\n", "            plot_intermediate_flow(flow, n_points=4000)\n", "    return flow\n", "\n", "flow = train(flow, X, epochs=epochs, bs=batch_size)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Final diagnostics"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["plot_data_and_density(flow, X, lim=3.0, grid_n=160)\n", "\n", "with torch.no_grad():\n", "    x_samp = flow.sample(6000).cpu().numpy()\n", "plt.figure(figsize=(4,4))\n", "plt.scatter(x_samp[:,0], x_samp[:,1], s=5, alpha=0.6)\n", "plt.title(\"Samples from trained flow\")\n", "plt.xlim(-3,3); plt.ylim(-3,3); plt.gca().set_aspect('equal', adjustable='box')\n", "plt.tight_layout(); plt.show()\n", "\n", "with torch.no_grad():\n", "    z_map, _ = flow.f(X)\n", "z_np = z_map[:6000].cpu().numpy()\n", "plt.figure(figsize=(4,4))\n", "plt.scatter(z_np[:,0], z_np[:,1], s=4, alpha=0.6)\n", "plt.title(\"Mapped data in base space (should be ~N(0,I))\")\n", "plt.xlim(-3,3); plt.ylim(-3,3); plt.gca().set_aspect('equal', adjustable='box')\n", "plt.tight_layout(); plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}