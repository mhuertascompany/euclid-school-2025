{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Score-based Diffusion (DSM + Annealed Langevin) on a 2D Two-Gaussian Mixture\n", "\n", "This mini-notebook trains a **noise-conditional score network** on a 2D **two-component Gaussian mixture** using **Denoising Score Matching (DSM)**, and visualizes:\n", "- the **estimated score field** \\( s_\\theta(x,\\sigma) \\) with arrows (quiver) over the true density,\n", "- **Langevin samples** at fixed noise levels \\( \\sigma \\),\n", "- and **Annealed Langevin Dynamics (ALD)** samples across a decreasing sequence of \\( \\sigma \\) values.\n", "\n", "The goal is **illustration and intuition**, not high performance."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1) Imports & setup"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import os, math, time\n", "import numpy as np\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import torch.optim as optim\n", "import matplotlib.pyplot as plt\n", "\n", "torch.manual_seed(0); np.random.seed(0)\n", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "print(\"device:\", device)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2) Target distribution: 2D two-component Gaussian mixture"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Mixture params\n", "w = torch.tensor([0.5, 0.5], dtype=torch.float32, device=device)              # weights\n", "mus = torch.tensor([[-2.0, 0.0], [ 2.0, 0.0]], dtype=torch.float32, device=device)  # means\n", "std = 0.5\n", "cov = (std**2) * torch.eye(2, device=device)  # isotropic\n", "\n", "def sample_mixture(n):\n", "    comp = torch.multinomial(w, num_samples=n, replacement=True)  # [n]\n", "    eps = torch.randn(n, 2, device=device) * std\n", "    x = mus[comp] + eps\n", "    return x\n", "\n", "def log_gaussian(x, mu, cov):\n", "    # cov is scalar*I here; handle generic form\n", "    d = x.size(-1)\n", "    inv = torch.inverse(cov)\n", "    diff = x - mu\n", "    quad = (diff @ inv * diff).sum(-1)\n", "    sign, logdet = torch.slogdet(cov)\n", "    return -0.5*(d*math.log(2*math.pi) + logdet + quad)\n", "\n", "def log_mixture(x):\n", "    # logsumexp over components\n", "    L = []\n", "    for k in range(mus.size(0)):\n", "        L.append(torch.log(w[k]) + log_gaussian(x, mus[k], cov))\n", "    return torch.logsumexp(torch.stack(L, dim=-1), dim=-1)\n", "\n", "@torch.no_grad()\n", "def true_score(x):\n", "    # \u2207_x log p(x) for a Gaussian mixture: weighted sum of component scores with responsibilities\n", "    # score of N(mu, cov) is cov^{-1}(mu - x)\n", "    inv = torch.inverse(cov)\n", "    logps = []\n", "    scores = []\n", "    for k in range(mus.size(0)):\n", "        logp_k = torch.log(w[k]) + log_gaussian(x, mus[k], cov)     # [B]\n", "        logps.append(logp_k)\n", "        scores.append((inv @ (mus[k] - x).unsqueeze(-1)).squeeze(-1))  # [B,2]\n", "    logps = torch.stack(logps, dim=-1)           # [B,K]\n", "    scores = torch.stack(scores, dim=-2)         # [B,K,2]\n", "    gamma = torch.softmax(logps, dim=-1)         # responsibilities [B,K]\n", "    s = (gamma.unsqueeze(-1) * scores).sum(dim=-2)  # [B,2]\n", "    return s\n", "\n", "# Visualize target samples\n", "with torch.no_grad():\n", "    x_demo = sample_mixture(2000).cpu().numpy()\n", "plt.figure(figsize=(4,4))\n", "plt.scatter(x_demo[:,0], x_demo[:,1], s=6, alpha=0.5)\n", "plt.title(\"Two-Gaussian mixture samples\"); plt.gca().set_aspect('equal', adjustable='box')\n", "plt.tight_layout(); plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3) DSM setup: noise levels and loss"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Noise levels for training: log-uniform in [sigma_min, sigma_max]\n", "sigma_min, sigma_max = 0.05, 1.0\n", "def sample_sigma(batch):\n", "    u = torch.rand(batch, device=device)\n", "    logs = torch.log(torch.tensor(sigma_min, device=device))*(1-u) + torch.log(torch.tensor(sigma_max, device=device))*u\n", "    return torch.exp(logs)\n", "\n", "def lambda_weight(sigma):  # common balancing\n", "    return sigma**2\n", "\n", "# Model: small MLP score network s_theta(x, sigma)\n", "class ScoreNet(nn.Module):\n", "    def __init__(self, hidden=128):\n", "        super().__init__()\n", "        self.net = nn.Sequential(\n", "            nn.Linear(2+1, hidden), nn.SiLU(),\n", "            nn.Linear(hidden, hidden), nn.SiLU(),\n", "            nn.Linear(hidden, 2)\n", "        )\n", "    def forward(self, x, sigma):\n", "        if sigma.dim()==1:\n", "            sigma = sigma.view(-1,1)\n", "        inp = torch.cat([x, torch.log(sigma)], dim=-1)  # condition on log sigma\n", "        return self.net(inp)\n", "\n", "score_net = ScoreNet(hidden=128).to(device)\n", "opt = optim.AdamW(score_net.parameters(), lr=2e-3)\n", "\n", "# Training data cache\n", "N_train = 100000\n", "train_data = sample_mixture(N_train)\n", "\n", "def dsm_loss(x):\n", "    B = x.size(0)\n", "    sigma = sample_sigma(B)\n", "    eps = torch.randn_like(x)\n", "    xt = x + sigma.view(-1,1) * eps\n", "    s = score_net(xt, sigma)                   # [B,2]\n", "    target = - eps / sigma.view(-1,1)          # [B,2]\n", "    w = lambda_weight(sigma).view(-1,1)\n", "    loss = 0.5 * w * (s - target)**2\n", "    return loss.sum(dim=1).mean()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4) Train the score model (DSM)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["batch_size = 1024\n", "steps = 2000\n", "print_every = 100\n", "\n", "loss_hist = []\n", "for it in range(1, steps+1):\n", "    idx = torch.randint(0, N_train, (batch_size,), device=device)\n", "    x = train_data[idx]\n", "    loss = dsm_loss(x)\n", "    opt.zero_grad(); loss.backward(); opt.step()\n", "    loss_hist.append(float(loss))\n", "    if it % print_every == 0:\n", "        print(f\"[{it:4d}/{steps}] loss={np.mean(loss_hist[-print_every:]):.4f}\")\n", "\n", "plt.figure(figsize=(5,3))\n", "plt.plot(loss_hist)\n", "plt.title(\"DSM training loss\"); plt.xlabel(\"step\"); plt.ylabel(\"loss\"); plt.tight_layout(); plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5) Visualize estimated score fields (quiver)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["@torch.no_grad()\n", "def plot_score_field(sigmas, lim=3.5, grid_n=25):\n", "    xs = np.linspace(-lim, lim, grid_n)\n", "    ys = np.linspace(-lim, lim, grid_n)\n", "    Xg, Yg = np.meshgrid(xs, ys)\n", "    grid = np.stack([Xg.ravel(), Yg.ravel()], axis=1)\n", "    grid_t = torch.tensor(grid, dtype=torch.float32, device=device)\n", "\n", "    # true density for background\n", "    lp = log_mixture(grid_t).cpu().numpy().reshape(grid_n, grid_n)\n", "\n", "    for s_val in sigmas:\n", "        sigma_t = torch.full((grid_t.size(0),), float(s_val), device=device)\n", "        s_est = score_net(grid_t, sigma_t).cpu().numpy()\n", "        U = s_est[:,0].reshape(grid_n, grid_n)\n", "        V = s_est[:,1].reshape(grid_n, grid_n)\n", "\n", "        plt.figure(figsize=(5,4))\n", "        plt.contourf(Xg, Yg, np.exp(lp - lp.max()), levels=30)  # normalized for display\n", "        plt.quiver(Xg, Yg, U, V, angles='xy', scale_units='xy', scale=10)\n", "        plt.scatter(x_demo[:,0], x_demo[:,1], s=6, alpha=0.25)\n", "        plt.title(f\"Estimated score field, sigma={s_val:.3f}\")\n", "        plt.gca().set_aspect('equal', adjustable='box')\n", "        plt.tight_layout(); plt.show()\n", "\n", "plot_score_field(sigmas=[1.0, 0.5, 0.2, 0.1], lim=3.5, grid_n=27)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6) Langevin sampling at fixed sigma (samples approximate q_sigma)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["@torch.no_grad()\n", "def langevin_fixed_sigma(n=2000, sigma=0.5, steps=80, step_size=None):\n", "    # Samples from the smoothed density q_sigma via Langevin dynamics\n", "    if step_size is None:\n", "        step_size = 0.1 * sigma**2\n", "    x = torch.randn(n, 2, device=device) * 2.0  # init wide\n", "    sig = torch.full((n,), float(sigma), device=device)\n", "    for _ in range(steps):\n", "        x = x + step_size * score_net(x, sig) + torch.sqrt(torch.tensor(2*step_size, device=device)) * torch.randn_like(x)\n", "    return x\n", "\n", "def plot_langevin_sigmas(sigmas, n=2000, steps=100):\n", "    cols = len(sigmas)\n", "    plt.figure(figsize=(4*cols,4))\n", "    for i, s in enumerate(sigmas):\n", "        x_samp = langevin_fixed_sigma(n=n, sigma=s, steps=steps).cpu().numpy()\n", "        ax = plt.subplot(1, cols, i+1)\n", "        ax.scatter(x_samp[:,0], x_samp[:,1], s=5, alpha=0.5, label=f\"sigma={s}\")\n", "        ax.scatter(x_demo[:,0], x_demo[:,1], s=5, alpha=0.25, label=\"data\")\n", "        ax.set_title(f\"Langevin @ sigma={s}\")\n", "        ax.set_xlim(-4,4); ax.set_ylim(-3,3); ax.set_aspect('equal', adjustable='box')\n", "        ax.legend(loc=\"upper right\")\n", "    plt.tight_layout(); plt.show()\n", "\n", "plot_langevin_sigmas(sigmas=[1.0, 0.5, 0.2, 0.1], n=1500, steps=120)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7) Annealed Langevin Dynamics (ALD) with sigma ladder"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["@torch.no_grad()\n", "def annealed_langevin(n=2000, sigmas=None, steps_per_sigma=60, step_coef=0.05):\n", "    if sigmas is None:\n", "        sigmas = np.geomspace(1.0, 0.05, 10)  # high to low\n", "    x = torch.randn(n, 2, device=device) * 2.0\n", "    traj = []  # store snapshots\n", "    for s in sigmas:\n", "        sig = torch.full((n,), float(s), device=device)\n", "        step_size = step_coef * (s**2)\n", "        for _ in range(steps_per_sigma):\n", "            x = x + step_size * score_net(x, sig) + torch.sqrt(torch.tensor(2*step_size, device=device)) * torch.randn_like(x)\n", "        traj.append(x.detach().cpu().numpy())\n", "    return sigmas, traj\n", "\n", "sigmas, traj = annealed_langevin(n=1500, sigmas=np.geomspace(1.0, 0.1, 6), steps_per_sigma=80, step_coef=0.05)\n", "\n", "# Plot snapshots\n", "plt.figure(figsize=(14,4))\n", "for i, (s, xs) in enumerate(zip(sigmas, traj)):\n", "    ax = plt.subplot(1, len(sigmas), i+1)\n", "    ax.scatter(xs[:,0], xs[:,1], s=5, alpha=0.5)\n", "    ax.set_title(f\"ALD snapshot\\nsigma={s:.3f}\")\n", "    ax.set_xlim(-4,4); ax.set_ylim(-3,3); ax.set_aspect('equal', adjustable='box')\n", "plt.tight_layout(); plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8) Notes\n", "- **DSM target:** \\(s_\\theta(\\tilde x,\\sigma) \\approx \\nabla_{\\tilde x}\\log q_\\sigma(\\tilde x)\\).\n", "- **Fixed-\\(\\sigma\\) Langevin** samples from the **smoothed** density \\(q_\\sigma\\).\n", "- **ALD** gradually reduces \\(\\sigma\\), sharpening samples toward the data distribution.\n", "- The score network conditions on \\(\\log\\sigma\\); richer embeddings (Fourier features) can improve quality.\n", "- Try different mixture parameters, sigma ladders, and step sizes for varied behavior."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}