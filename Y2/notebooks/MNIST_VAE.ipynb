{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "769d1577",
   "metadata": {},
   "source": [
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/mhuertascompany/euclid-school-2025/blob/main/Y2/notebooks/MNIST_VAE.ipynb)\n",
    "\n",
    "# A Minimal, Explicit Variational Autoencoder (VAE)\n",
    "\n",
    "## Rodolphe Cledassou School 2025\n",
    "\n",
    "> Marc Huertas-Company & Hubert Bretonnière \n",
    "\n",
    "**Goal:** teach the VAE objective and implementation with clear, *separable* loss terms (reconstruction and KL).  \n",
    "We’ll train a simple VAE on MNIST with either a Bernoulli (BCE) or Gaussian (MSE) decoder and inspect each term.\n",
    "\n",
    "**You will see:**\n",
    "- The ELBO written explicitly and how each term maps to code\n",
    "- The reparameterization trick\n",
    "- A clean training loop logging `recon_loss`, `kl_loss`, and `elbo`\n",
    "- Sampling from the prior and visualizing reconstructions / latent space\n",
    "\n",
    "---\n",
    "**ELBO (per-sample):**\n",
    "$$\\begin{equation}\n",
    "\\mathcal{L}(\\theta,\\phi;x)\n",
    "= \\mathbb{E}_{q_\\phi(z\\mid x)}\\big[\\log p_\\theta(x\\mid z)\\big]\n",
    "- \\mathrm{KL}\\!\\big(q_\\phi(z\\mid x)\\,\\|\\,p(z)\\big).\n",
    "\\end{equation}$$\n",
    "\n",
    "**VAE loss (minimize negative ELBO):**\n",
    "$$\\begin{equation}\n",
    "\\mathcal{L}_\\text{VAE}(x)\n",
    "= -\\,\\mathbb{E}_{q_\\phi}\\log p_\\theta(x\\mid z)\n",
    "+ \\mathrm{KL}\\!\\big(q_\\phi(z\\mid x)\\,\\|\\,p(z)\\big).\n",
    "\\end{equation}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19704809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EUCLID SCHOOL: LIGHT BOOTSTRAP (no data) -------------------------------\n",
    "# Detect Colab, (optionally) install minimal deps, (optionally) clone the repo,\n",
    "# and print device info. It does NOT download any dataset.\n",
    "# ----------------------------------------------------------------------------\n",
    "import os, sys, subprocess\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "# --- Colab detection\n",
    "IN_COLAB = False\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# --- Settings (edit if needed)\n",
    "INSTALL_DEPS = True                # set False if you want to skip pip installs on Colab\n",
    "PIP_PKGS = [\n",
    "    # Keep small. Colab already has torch + CUDA.\n",
    "    \"datasets==4.*\", \"transformers==4.*\", \"timm==1.*\", \"albumentations==2.*\",\n",
    "    \"lightning==2.*\", \"pytorch-lightning==2.*\", \"einops==0.*\",\n",
    "    \"pyarrow\", \"seaborn\", \"umap-learn\", \"nflows\",\n",
    "    \"tensorboard\", \"tqdm\", \"safetensors\", \"opencv-python\"\n",
    "]\n",
    "\n",
    "# If your notebook relies on repo-relative paths, you can enable this:\n",
    "CLONE_REPO = False                 # set True only if needed\n",
    "REPO_URL   = \"https://github.com/mhuertascompany/euclid-school-2025.git\"\n",
    "REPO_DIR   = \"/content/euclid-school-2025\"\n",
    "SUBDIR     = None                  # e.g., \"Y1/notebooks\" or \"Y2/xyz\"\n",
    "\n",
    "def pip_install(pkgs):\n",
    "    if not pkgs: return\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\"] + list(pkgs)\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running on Google Colab ✓\")\n",
    "    if INSTALL_DEPS:\n",
    "        print(\"Installing minimal pip deps…\")\n",
    "        pip_install(PIP_PKGS)\n",
    "\n",
    "    if CLONE_REPO:\n",
    "        if not os.path.isdir(REPO_DIR):\n",
    "            print(f\"Cloning {REPO_URL} …\")\n",
    "            subprocess.run([\"git\", \"clone\", \"-q\", REPO_URL, REPO_DIR], check=True)\n",
    "        if SUBDIR:\n",
    "            os.chdir(os.path.join(REPO_DIR, SUBDIR))\n",
    "            print(\"Working directory:\", os.getcwd())\n",
    "\n",
    "    # Device info\n",
    "    try:\n",
    "        subprocess.run([\"nvidia-smi\"], check=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    print(\"Not running on Colab (no action).\")\n",
    "# ----------------------------------------------------------------------------\n",
    "import torch\n",
    "print(\"\\nPyTorch:\", torch.__version__)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else\n",
    "                      \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install torch torchvision  # uncomment if needed\n",
    "\n",
    "import math, os, time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "latent_dim = 2\n",
    "hidden_dim = 512\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "lr = 1e-3\n",
    "decoder_likelihood = \"gaussian\"  # 'bernoulli' or 'gaussian'\n",
    "fixed_gaussian_sigma = 0.1\n",
    "beta = 1.0\n",
    "\n",
    "save_dir = \"vae_outputs\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d01de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_ds = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "test_ds  = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "print(\"Train size:\", len(train_ds), \" Test size:\", len(test_ds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7ea236",
   "metadata": {},
   "source": [
    "\n",
    "## Model\n",
    "MLP encoder/decoder:\n",
    "- Encoder: $$x \\mapsto (\\mu_\\phi(x), \\log\\sigma^2_\\phi(x))$$\n",
    "- Reparameterization: $$z=\\mu+\\sigma\\odot\\varepsilon,\\ \\varepsilon\\sim\\mathcal N(0,I)$$\n",
    "- Decoder: $$z \\mapsto \\hat x$$ (logits for Bernoulli; mean for Gaussian)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fff276",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=2, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.to_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.to_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "    def forward(self, x):\n",
    "        h = self.net(x)\n",
    "        mu = self.to_mu(h)\n",
    "        logvar = self.to_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=2, hidden_dim=512, likelihood=\"bernoulli\"):\n",
    "        super().__init__()\n",
    "        self.likelihood = likelihood\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.to_out = nn.Linear(hidden_dim, 28*28)\n",
    "    def forward(self, z):\n",
    "        h = self.net(z)\n",
    "        x_flat = self.to_out(h)\n",
    "        return x_flat.view(-1, 1, 28, 28)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=2, hidden_dim=512, likelihood=\"bernoulli\"):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(latent_dim, hidden_dim)\n",
    "        self.dec = Decoder(latent_dim, hidden_dim, likelihood)\n",
    "        self.likelihood = likelihood\n",
    "    @staticmethod\n",
    "    def reparameterize(mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.enc(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_logits_or_mean = self.dec(z)\n",
    "        return x_logits_or_mean, mu, logvar, z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc8212c",
   "metadata": {},
   "source": [
    "\n",
    "## Loss terms (explicit)\n",
    "\n",
    "**KL (diag Gaussian $$q$$ vs $$N(0,I)$$)**\n",
    "$$\\mathrm{KL} = -\\tfrac{1}{2}\\sum_j \\big(1+\\log\\sigma_j^2 - \\mu_j^2 - \\sigma_j^2\\big).$$\n",
    "\n",
    "**Reconstruction**\n",
    "- Bernoulli: `BCEWithLogitsLoss` gives $$-\\log p_\\theta(x|z)$$ summed over pixels.\n",
    "- Gaussian (fixed $$\\sigma^2$$): use $$\\frac{1}{2\\sigma^2}\\|x-\\mu_\\theta(z)\\|^2$$ (dropping constants).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f274a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kl_divergence_diag_gaussian(mu, logvar):\n",
    "    return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "\n",
    "bce = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "def recon_loss(x_logits_or_mean, x, likelihood=\"bernoulli\", sigma=0.1):\n",
    "    if likelihood == \"bernoulli\":\n",
    "        return bce(x_logits_or_mean, x)\n",
    "    elif likelihood == \"gaussian\":\n",
    "        mse = F.mse_loss(x_logits_or_mean, x, reduction='sum')\n",
    "        return (1.0 / (2 * sigma * sigma)) * mse\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "def elbo_loss(x, x_logits_or_mean, mu, logvar, likelihood=\"bernoulli\", beta=1.0, sigma=0.1):\n",
    "    rl = recon_loss(x_logits_or_mean, x, likelihood, sigma)\n",
    "    kl = kl_divergence_diag_gaussian(mu, logvar).sum()\n",
    "    elbo = - rl - beta * kl\n",
    "    return rl, kl, elbo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca806d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vae = VAE(latent_dim=latent_dim, hidden_dim=hidden_dim, likelihood=decoder_likelihood).to(device)\n",
    "opt = torch.optim.Adam(vae.parameters(), lr=lr)\n",
    "\n",
    "def train_one_epoch(epoch):\n",
    "    vae.train()\n",
    "    total_rl, total_kl, total_elbo = 0.0, 0.0, 0.0\n",
    "    for x, _ in train_loader:\n",
    "        x = x.to(device)\n",
    "        x_logits_or_mean, mu, logvar, z = vae(x)\n",
    "        rl, kl, elbo = elbo_loss(x, x_logits_or_mean, mu, logvar,\n",
    "                                 likelihood=decoder_likelihood, beta=beta, sigma=fixed_gaussian_sigma)\n",
    "        loss = -elbo\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_rl += rl.item()\n",
    "        total_kl += kl.item()\n",
    "        total_elbo += elbo.item()\n",
    "    n = len(train_loader.dataset)\n",
    "    print(f\"[Epoch {epoch}] recon={total_rl/n:.3f}  kl={total_kl/n:.3f}  -elbo={-total_elbo/n:.3f}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(split=\"test\", save=True):\n",
    "    vae.eval()\n",
    "    loader = test_loader if split==\"test\" else train_loader\n",
    "    total_rl, total_kl, total_elbo = 0.0, 0.0, 0.0\n",
    "    for x, _ in loader:\n",
    "        x = x.to(device)\n",
    "        x_logits_or_mean, mu, logvar, z = vae(x)\n",
    "        rl, kl, elbo = elbo_loss(x, x_logits_or_mean, mu, logvar,\n",
    "                                 likelihood=decoder_likelihood, beta=beta, sigma=fixed_gaussian_sigma)\n",
    "        total_rl += rl.item(); total_kl += kl.item(); total_elbo += elbo.item()\n",
    "    n = len(loader.dataset)\n",
    "    print(f\"[{split.upper()}] recon={total_rl/n:.3f}  kl={total_kl/n:.3f}  -elbo={-total_elbo/n:.3f}\")\n",
    "\n",
    "    if save:\n",
    "        # save reconstructions and prior samples\n",
    "        from torchvision import utils\n",
    "        x, _ = next(iter(loader))\n",
    "        x = x.to(device)[:64]\n",
    "        x_logits_or_mean, _, _, _ = vae(x)\n",
    "        if decoder_likelihood == \"bernoulli\":\n",
    "            x_recon = torch.sigmoid(x_logits_or_mean)\n",
    "        else:\n",
    "            x_recon = x_logits_or_mean.clamp(0,1)\n",
    "        grid_true = utils.make_grid(x.cpu(), nrow=8)\n",
    "        grid_reco = utils.make_grid(x_recon.cpu(), nrow=8)\n",
    "        utils.save_image(grid_true, os.path.join(save_dir, \"true_grid.png\"))\n",
    "        utils.save_image(grid_reco, os.path.join(save_dir, \"reco_grid.png\"))\n",
    "        z = torch.randn(64, latent_dim, device=device)\n",
    "        x_logits_or_mean = vae.dec(z)\n",
    "        if decoder_likelihood == \"bernoulli\":\n",
    "            x_sample = torch.sigmoid(x_logits_or_mean)\n",
    "        else:\n",
    "            x_sample = x_logits_or_mean.clamp(0,1)\n",
    "        grid_samp = utils.make_grid(x_sample.cpu(), nrow=8)\n",
    "        utils.save_image(grid_samp, os.path.join(save_dir, \"samples_grid.png\"))\n",
    "        print(\"Saved true_grid.png, reco_grid.png, samples_grid.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e8d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_one_epoch(epoch)\n",
    "    evaluate(\"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f1922",
   "metadata": {},
   "source": [
    "\n",
    "## Latent space plot (for \\(d_z=2\\))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a9551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_latent_space(split=\"test\", max_points=5000):\n",
    "    vae.eval()\n",
    "    loader = test_loader if split==\"test\" else train_loader\n",
    "    mus, ys, count = [], [], 0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        mu, logvar = vae.enc(x)\n",
    "        mus.append(mu.cpu()); ys.append(y)\n",
    "        count += x.size(0)\n",
    "        if count >= max_points: break\n",
    "    mu_all = torch.cat(mus, dim=0).numpy()\n",
    "    y_all = torch.cat(ys, dim=0).numpy()\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sc = plt.scatter(mu_all[:,0], mu_all[:,1], c=y_all, s=5, cmap=\"tab10\")\n",
    "    plt.colorbar(sc, ticks=range(10))\n",
    "    plt.title(\"Latent means μ(x)\"); plt.xlabel(\"z1\"); plt.ylabel(\"z2\")\n",
    "    out = os.path.join(save_dir, \"latent_scatter.png\")\n",
    "    plt.tight_layout(); plt.savefig(out, dpi=150); plt.close()\n",
    "    print(\"Saved\", out)\n",
    "    return mus, y_all\n",
    "\n",
    "mus, ys = plot_latent_space()\n",
    "mus  = np.reshape(mus, (40*128, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7389b9c2",
   "metadata": {},
   "source": [
    "## Interactive plotting of new numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4543b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Explore_ls:\n",
    "    def __init__(\n",
    "        self, \n",
    "        fig, \n",
    "        ax,\n",
    "        labels,\n",
    "        codes\n",
    "    ):\n",
    "\n",
    "        self.plot_ls = ax[0]  # define the left subplot, where the latent space is plotted\n",
    "        self.new_samples = [] \n",
    "        self.image = ax[1]     # define the right subplot, where the image is created\n",
    "        self.codes = codes\n",
    "        fig.canvas.mpl_connect('button_press_event', self)\n",
    "\n",
    "        # plot the latent space\n",
    "        self.ls_plot = self.plot_ls.scatter(codes[:, 0],codes[:, 1],c=labels, cmap='tab10')\n",
    "        self.plot_ls.set_xlabel('z1')\n",
    "        self.plot_ls.set_ylabel('z2')\n",
    "\n",
    "        # Reserve some space on top for the title\n",
    "        fig.subplots_adjust(top=0.8)\n",
    "\n",
    "    def __call__(self, event):\n",
    "\n",
    "        click_x = event.xdata\n",
    "        click_y = event.ydata\n",
    "        self.plot_ls.set_title(click_x)\n",
    "        if event.button == 1:\n",
    "        \n",
    "            # Create and plot the sample in the LS\n",
    "            self.new_samples.append(self.plot_ls.scatter(\n",
    "                event.xdata, \n",
    "                event.ydata, \n",
    "                marker='*',  # change the marker style to distinguish between sample from the encoded digits\n",
    "                c='red',     # and the new samples\n",
    "            ))\n",
    "            \n",
    "            # Add info to the title\n",
    "            self.plot_ls.set_title(f\"clicked (z0, z1): ({click_x:.2f}, {click_y:.2f})\")\n",
    "            \n",
    "            #sample the vector to feed the decoder\n",
    "            sample = np.reshape([click_x, click_y], (1, 2))\n",
    "            sample_tensor = torch.from_numpy(sample).to(torch.float32).to(device)\n",
    "            # feed and run the decoder\n",
    "            decoded = vae.dec(sample_tensor)\n",
    "            # Show the decoded image\n",
    "            self.image.imshow(decoded[0, 0, :, :].cpu().detach().numpy(), cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e525d450",
   "metadata": {},
   "source": [
    "## if not installed yet, install ipyml for interactiveplotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc17e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f407c585",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b47c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "interact = Explore_ls(fig, ax, ys, mus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c58cb53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "euclid-school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
