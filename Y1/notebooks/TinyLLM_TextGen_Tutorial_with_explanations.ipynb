{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# TinyLLM: A Small Transformer for Text Generation (PyTorch, HF Datasets)\n", "\n", "This notebook trains a **tiny Transformer decoder** (a minimal LLM) for **character-level** text generation using a small dataset from Hugging Face. It is designed to be **simple, fast, and pedagogical**, so it can run on a laptop (CPU or Mac MPS).\n", "\n", "What you'll see:\n", "- Loading a tiny dataset (**Tiny Shakespeare**).\n", "- Building a **char-level tokenizer** from scratch.\n", "- Implementing a **Transformer decoder** (multi-head self-attention + MLP).\n", "- Training with cross-entropy next-token prediction.\n", "- **Sampling** with temperature and top-*k*.\n", "- **Attention map** visualizations for interpretability."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 0) Setup (device & optional installs)\n", "\n", "If you don't have the `datasets` library, uncomment the first cell to install."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**What this cell does (Setup & Device):**\n", "- Imports core libraries (`torch`, `datasets`) and picks a compute **device**:\n", "  - Prefer **MPS** (Apple Metal) on Macs, then **CUDA**, else **CPU**.\n", "- Sets **random seeds** for reproducibility.\n", "- If `datasets` is missing, you can `pip install datasets` (comment in the cell).\n", "**Key ideas:** you must move both **model** and **batches** to the same device."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# If needed, uncomment:\n", "# %pip install -q datasets\n", "\n", "import math, time, random, os, sys\n", "import numpy as np\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "from datasets import load_dataset\n", "\n", "device = (\n", "    torch.device(\"mps\") if torch.backends.mps.is_available()\n", "    else torch.device(\"cuda\") if torch.cuda.is_available()\n", "    else torch.device(\"cpu\")\n", ")\n", "print(\"Using device:\", device)\n", "\n", "torch.manual_seed(42); np.random.seed(42); random.seed(42)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1) Load a tiny dataset (Tiny Shakespeare)\n", "\n", "We use the **`tiny_shakespeare`** dataset (about 1 MB). It's small but has rich structure to learn character-level language modeling.\n", "\n", "- We concatenate the text into one long string.\n", "- Then we will split into **train/val** segments."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**What this cell does (Data loading):**\n", "- Downloads the **Tiny Shakespeare** corpus from \ud83e\udd17 Datasets.\n", "- Concatenates all text chunks into a single string `raw_text` for easy indexing.\n", "**Why:** character-level LM treats the entire corpus as one long stream of tokens.\n", "**Tip:** If `tiny_shakespeare` fails on your setup, switch to the generic text loader with a raw URL."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["ds = load_dataset(\"tiny_shakespeare\")\n", "all_texts = ds[\"train\"][\"text\"]\n", "raw_text = \"\\n\".join(all_texts)\n", "print(\"Raw text size (chars):\", len(raw_text))\n", "print(raw_text[:500])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2) Character vocabulary & encoding/decoding\n", "\n", "For pedagogy, we do **character-level** modeling:\n", "- Build `vocab` as the sorted unique characters in the dataset.\n", "- Create `stoi` / `itos` maps to encode/decode between text and integer IDs."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**What this cell does (Character vocabulary):**\n", "- Builds a **character-level vocabulary** from unique chars in the corpus.\n", "- Creates maps:\n", "  - `stoi` (string\u2192id) to **encode** text to integers.\n", "  - `itos` (id\u2192string) to **decode** integers back to text.\n", "**Why:** this avoids an external tokenizer and keeps the tutorial minimal."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["chars = sorted(list(set(raw_text)))\n", "vocab_size = len(chars)\n", "print(\"Vocab size:\", vocab_size)\n", "print(\"First 100 chars of vocab:\", \"\".join(chars[:100]))\n", "\n", "stoi = {ch:i for i,ch in enumerate(chars)}\n", "itos = {i:ch for ch,i in stoi.items()}\n", "\n", "def encode(s: str):\n", "    return [stoi[c] for c in s]\n", "\n", "def decode(ids):\n", "    return \"\".join(itos[i] for i in ids)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3) Train/Val split and batching\n", "\n", "- Convert the entire text into a **1D tensor of token IDs**.\n", "- Split into **90% train** and **10% val**.\n", "- Define a `get_batch(split)` function that samples random contiguous blocks of length **`block_size`**."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**What this cell does (Train/Val split & batching):**\n", "- Converts the entire text into a 1D tensor of token IDs.\n", "- Splits into **90% train** / **10% validation**.\n", "- Defines `block_size` (context window) and `batch_size`.\n", "- Implements `get_batch(split)` that samples random **contiguous** windows:\n", "  - `x` is tokens `[t, t+1, ..., t+block_size-1]`\n", "  - `y` is next tokens `[t+1, ..., t+block_size]`\n", "**Why:** next-token prediction trains the model to predict the next character given the previous **context**."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["data = torch.tensor(encode(raw_text), dtype=torch.long)\n", "\n", "n = int(0.9 * len(data))\n", "train_ids = data[:n]\n", "val_ids   = data[n:]\n", "\n", "block_size = 128\n", "batch_size = 64\n", "print(f\"Train tokens: {train_ids.numel():,} | Val tokens: {val_ids.numel():,}\")\n", "\n", "def get_batch(split):\n", "    ids = train_ids if split == \"train\" else val_ids\n", "    ix = torch.randint(0, len(ids) - block_size - 1, (batch_size,))\n", "    x = torch.stack([ids[i:i+block_size] for i in ix])\n", "    y = torch.stack([ids[i+1:i+block_size+1] for i in ix])\n", "    return x.to(device), y.to(device)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4) Tiny Transformer decoder (from scratch)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**What this cell does (Tiny Transformer model):**\n", "- Implements a **GPT-style** decoder block from scratch:\n", "  - **Token & positional embeddings** (learned).\n", "  - **Causal multi-head self-attention** with a **lower-triangular mask** so tokens cannot see the future.\n", "  - **Pre-LayerNorm** + **MLP** with GELU and residual connections.\n", "- The attention step computes `softmax(QK\u1d40/\u221ad)\u00b7V`, then projects back to the model dimension.\n", "**Outputs:** logits over the vocabulary for each position; if `targets` given, returns **cross-entropy loss**."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["class CausalSelfAttention(nn.Module):\n", "    def __init__(self, n_embd, n_head, dropout, block_size):\n", "        super().__init__()\n", "        assert n_embd % n_head == 0\n", "        self.n_head = n_head\n", "        self.head_dim = n_embd // n_head\n", "\n", "        self.q_proj = nn.Linear(n_embd, n_embd)\n", "        self.k_proj = nn.Linear(n_embd, n_embd)\n", "        self.v_proj = nn.Linear(n_embd, n_embd)\n", "        self.out_proj = nn.Linear(n_embd, n_embd)\n", "        self.attn_drop = nn.Dropout(dropout)\n", "        self.resid_drop = nn.Dropout(dropout)\n", "\n", "        mask = torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size)\n", "        self.register_buffer(\"mask\", mask)\n", "\n", "    def forward(self, x, return_attn=False):\n", "        B, T, C = x.shape\n", "        q = self.q_proj(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n", "        k = self.k_proj(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n", "        v = self.v_proj(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n", "\n", "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n", "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n", "        att = F.softmax(att, dim=-1)\n", "        att = self.attn_drop(att)\n", "\n", "        y = att @ v\n", "        y = y.transpose(1, 2).contiguous().view(B, T, -1)\n", "        y = self.resid_drop(self.out_proj(y))\n", "        if return_attn:\n", "            return y, att\n", "        return y\n", "\n", "class Block(nn.Module):\n", "    def __init__(self, n_embd, n_head, dropout, block_size):\n", "        super().__init__()\n", "        self.ln1 = nn.LayerNorm(n_embd)\n", "        self.sa = CausalSelfAttention(n_embd, n_head, dropout, block_size)\n", "        self.ln2 = nn.LayerNorm(n_embd)\n", "        self.mlp = nn.Sequential(\n", "            nn.Linear(n_embd, 4*n_embd),\n", "            nn.GELU(),\n", "            nn.Linear(4*n_embd, n_embd),\n", "            nn.Dropout(dropout),\n", "        )\n", "    def forward(self, x, return_attn=False):\n", "        if return_attn:\n", "            y, att = self.sa(self.ln1(x), return_attn=True)\n", "            x = x + y\n", "            x = x + self.mlp(self.ln2(x))\n", "            return x, att\n", "        else:\n", "            x = x + self.sa(self.ln1(x))\n", "            x = x + self.mlp(self.ln2(x))\n", "            return x\n", "\n", "class TinyTransformer(nn.Module):\n", "    def __init__(self, vocab_size, n_embd=128, n_head=4, n_layer=4, block_size=128, dropout=0.1):\n", "        super().__init__()\n", "        self.block_size = block_size\n", "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n", "        self.pos_emb = nn.Embedding(block_size, n_embd)\n", "        self.drop = nn.Dropout(dropout)\n", "        self.blocks = nn.ModuleList([Block(n_embd, n_head, dropout, block_size) for _ in range(n_layer)])\n", "        self.ln_f = nn.LayerNorm(n_embd)\n", "        self.head = nn.Linear(n_embd, vocab_size)\n", "        self.head.weight = self.tok_emb.weight  # weight tying\n", "\n", "    def forward(self, idx, targets=None, return_attn=False):\n", "        B, T = idx.shape\n", "        assert T <= self.block_size\n", "        tok = self.tok_emb(idx)\n", "        pos = self.pos_emb(torch.arange(T, device=idx.device))\n", "        x = self.drop(tok + pos)\n", "\n", "        attn_maps = [] if return_attn else None\n", "        for blk in self.blocks:\n", "            if return_attn:\n", "                x, att = blk(x, return_attn=True)\n", "                attn_maps.append(att)\n", "            else:\n", "                x = blk(x)\n", "        x = self.ln_f(x)\n", "        logits = self.head(x)\n", "        loss = None\n", "        if targets is not None:\n", "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n", "        if return_attn:\n", "            return logits, loss, attn_maps\n", "        return logits, loss"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Hyperparameters & model init"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**What this cell does (Hyperparameters & initialization):**\n", "- Sets small, laptop-friendly sizes: embedding dim, heads, layers, dropout.\n", "- Instantiates the model and an **AdamW** optimizer.\n", "- Uses **weight tying** (output head shares weights with token embeddings) to reduce parameters and help training.\n", "**Rule of thumb:** larger `block_size` \u21d2 longer context but more compute; start small and scale up gradually."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["n_embd = 128; n_head = 4; n_layer = 4; dropout = 0.1\n", "model = TinyTransformer(vocab_size, n_embd, n_head, n_layer, block_size, dropout).to(device)\n", "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\n", "print(\"Parameters (M):\", sum(p.numel() for p in model.parameters())/1e6)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5) Training loop"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**What this cell does (Training loop):**\n", "- Trains for a few hundred steps with **cross-entropy** next-token loss.\n", "- Every `eval_interval`, runs a quick **validation** estimate (no grad).\n", "- Uses **gradient clipping** to avoid exploding gradients.\n", "**Watch:** training loss should go down; validation loss may be a bit higher."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["max_steps = 800\n", "eval_interval = 100\n", "log_every = 50\n", "\n", "def estimate_loss():\n", "    model.eval()\n", "    out = {}\n", "    with torch.no_grad():\n", "        for split in [\"train\", \"val\"]:\n", "            losses = []\n", "            for _ in range(20):\n", "                xb, yb = get_batch(\"train\" if split==\"train\" else \"val\")\n", "                _, loss = model(xb, yb)\n", "                losses.append(loss.item())\n", "            out[split] = sum(losses)/len(losses)\n", "    model.train()\n", "    return out\n", "\n", "model.train()\n", "for step in range(1, max_steps+1):\n", "    xb, yb = get_batch(\"train\")\n", "    logits, loss = model(xb, yb)\n", "    optimizer.zero_grad(set_to_none=True)\n", "    loss.backward()\n", "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n", "    optimizer.step()\n", "\n", "    if step % log_every == 0:\n", "        print(f\"step {step:4d} | train loss {loss.item():.3f}\")\n", "    if step % eval_interval == 0 or step == max_steps:\n", "        losses = estimate_loss()\n", "        print(f\"[eval] step {step} | train {losses['train']:.3f} | val {losses['val']:.3f}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6) Sampling (temperature & top-k)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**What this cell does (Autoregressive sampling):**\n", "- Implements greedy/tempered **sampling**:\n", "  - Crops to the last `block_size` tokens (context window).\n", "  - Applies **temperature** (higher \u21d2 more randomness).\n", "  - Optional **top-k** filtering keeps only the top-k logits (sharper outputs).\n", "- Generates new characters step-by-step appended to the prompt.\n", "**Tip:** try different temperatures (e.g., 0.7, 1.0, 1.3) and `top_k` (e.g., 20, 40) to see the effect."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["@torch.no_grad()\n", "def generate(model, idx, max_new_tokens=200, temperature=1.0, top_k=None):\n", "    model.eval()\n", "    for _ in range(max_new_tokens):\n", "        idx_cond = idx[:, -model.block_size:]\n", "        logits, _ = model(idx_cond)\n", "        logits = logits[:, -1, :] / max(1e-8, temperature)\n", "        if top_k is not None:\n", "            v, _ = torch.topk(logits, top_k)\n", "            logits[logits < v[:, [-1]]] = -float('inf')\n", "        probs = F.softmax(logits, dim=-1)\n", "        next_id = torch.multinomial(probs, num_samples=1)\n", "        idx = torch.cat([idx, next_id], dim=1)\n", "    return idx\n", "\n", "prompt = \"ROMEO:\"\n", "prompt_ids = torch.tensor([ [stoi[c] for c in prompt] ], dtype=torch.long, device=device)\n", "gen_ids = generate(model, prompt_ids, max_new_tokens=300, temperature=0.9, top_k=40)\n", "print(\"=== SAMPLE ===\")\n", "print(\"\".join(itos[i] for i in gen_ids[0].tolist()))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7) Attention maps (visualization)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**What this cell does (Attention visualization):**\n", "- Runs a short prompt through the model while collecting **attention weights**.\n", "- Plots one **head** of one **layer** as a heatmap:\n", "  - Rows = **query** positions (current token).\n", "  - Columns = **key** positions (context tokens).\n", "  - Brighter = higher attention weight.\n", "**Why:** attention maps give intuition about which characters/positions the model focuses on."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "\n", "model.eval()\n", "with torch.no_grad():\n", "    sample_text = \"ROMEO:\\n\"\n", "    idx = torch.tensor([[stoi[c] for c in sample_text]], dtype=torch.long, device=device)\n", "    idx = idx[:, :min(idx.size(1), model.block_size)]\n", "    logits, loss, attn_maps = model(idx, targets=None, return_attn=True)\n", "\n", "layer_to_show = 0\n", "head_to_show = 0\n", "att = attn_maps[layer_to_show][0, head_to_show].cpu().numpy()\n", "tokens = list(sample_text[:att.shape[0]])\n", "\n", "plt.figure(figsize=(6,5))\n", "plt.imshow(att)\n", "plt.xticks(range(len(tokens)), tokens, rotation=90)\n", "plt.yticks(range(len(tokens)), tokens)\n", "plt.xlabel(\"Key positions\"); plt.ylabel(\"Query positions\")\n", "plt.title(f\"Attention \u2014 layer {layer_to_show}, head {head_to_show}\")\n", "plt.tight_layout(); plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8) Next steps\n", "- Try more steps or a slightly larger model.\n", "- Switch to **subword tokenization** (BPE) for more realistic language modeling.\n", "- Add **learning rate schedules**, **dropout tuning**, or **checkpointing**.\n", "- Visualize attention across different layers/heads and for different prompts."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}