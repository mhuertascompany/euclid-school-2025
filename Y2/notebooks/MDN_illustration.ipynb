{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Intro: Neural Networks for Log-Probability (Fully PyTorch)\n", "\n", "We\u2019ll estimate conditional densities \\(p(y\\mid x)\\) with three progressively richer models:\n", "1. **Deterministic regression** trained by MSE (predict a point estimate).\n", "2. **Gaussian regression** that outputs a Normal distribution \\( \\mathcal N(\\mu(x),\\sigma(x)^2) \\) and is trained by **negative log-likelihood (NLL)**.\n", "3. **Mixture Density Network (MDN)** that outputs a Gaussian mixture to capture **multi-modality/heteroscedasticity**.\n", "\n", "Along the way we\u2019ll:\n", "- Compare **MSE vs. NLL** objectives.\n", "- Visualize **predictive mean/uncertainty**.\n", "- Plot the learned **conditional density** \\(p(y\\mid x)\\)."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import numpy as np\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import torch.optim as optim\n", "import torch.distributions as D\n", "import matplotlib.pyplot as plt\n", "\n", "torch.manual_seed(0)\n", "np.random.seed(0)\n", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "print(\"device:\", device)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1) Toy data: non-linear, multimodal, heteroscedastic\n", "\n", "We design \\( y\\mid x \\) to be **multimodal** for some \\(x\\) (two possible branches) and with **changing noise**."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Generate training data\n", "N = 2000\n", "x = np.random.uniform(-4.0, 4.0, size=(N,1)).astype(np.float32)\n", "\n", "# Conditional: two branches around a sine curve + a linear branch, with x-dependent noise\n", "def sample_y(xv, rng):\n", "    # Mix probability depends on x\n", "    w = 1/(1 + np.exp(-1.0*xv))  # smoothly increases with x\n", "    # Branch 1: sinusoid with small noise\n", "    y1 = np.sin(1.5*xv) + 0.15 * rng.standard_normal(size=xv.shape)\n", "    # Branch 2: linear with larger noise\n", "    y2 = 0.5*xv + 1.0 + 0.6 * rng.standard_normal(size=xv.shape)\n", "    choose = rng.uniform(size=xv.shape) < w\n", "    return np.where(choose, y1, y2)\n", "\n", "rng = np.random.default_rng(0)\n", "y = sample_y(x, rng).astype(np.float32)\n", "\n", "x_t = torch.tensor(x, device=device)\n", "y_t = torch.tensor(y, device=device)\n", "\n", "# Train/test grids\n", "x_grid = np.linspace(-4,4,400, dtype=np.float32).reshape(-1,1)\n", "xg_t = torch.tensor(x_grid, device=device)\n", "\n", "plt.figure(figsize=(5,4))\n", "plt.scatter(x[:1000,0], y[:1000,0], s=8, alpha=0.4)\n", "plt.title(\"Toy training data\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.tight_layout(); plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2) Common helpers"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def batches(X, Y, bs=256):\n", "    N = X.shape[0]\n", "    idx = torch.randperm(N, device=X.device)\n", "    for i in range(0, N, bs):\n", "        j = idx[i:i+bs]\n", "        yield X[j], Y[j]\n", "\n", "@torch.no_grad()\n", "def plot_regression(x, y, x_grid, mean, std=None, title=\"\"):\n", "    plt.figure(figsize=(6,4))\n", "    plt.scatter(x[:1500,0].cpu(), y[:1500,0].cpu(), s=8, alpha=0.35, label=\"data\")\n", "    plt.plot(x_grid[:,0].cpu(), mean.cpu(), lw=2, label=\"mean\")\n", "    if std is not None:\n", "        m = mean.cpu().numpy().reshape(-1)\n", "        s = std.cpu().numpy().reshape(-1)\n", "        up = m + 2*s; lo = m - 2*s\n", "        plt.fill_between(x_grid[:,0].cpu(), lo, up, alpha=0.25, label=\"\u00b12\u03c3\")\n", "    plt.title(title); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.tight_layout(); plt.show()\n", "\n", "@torch.no_grad()\n", "def plot_density(x_grid, y_lims=(-3.5,3.5), steps=200, logp_fn=None, title=\"p(y|x)\"):\n", "    y_lin = torch.linspace(y_lims[0], y_lims[1], steps, device=x_grid.device).view(1,-1)\n", "    X = x_grid  # [G,1]\n", "    Y = y_lin.repeat(X.size(0),1)  # [G,S]\n", "    # Evaluate log p(y|x) for each grid pair\n", "    lp = logp_fn(X, Y)  # should return [G,S]\n", "    p = lp.exp().cpu().numpy()\n", "    Xg, Yg = np.meshgrid(X[:,0].cpu().numpy(), y_lin[0].cpu().numpy())\n", "    plt.figure(figsize=(6,4))\n", "    plt.contourf(Xg, Yg, p.T, levels=40)\n", "    plt.colorbar(label=\"p(y|x)\")\n", "    plt.title(title); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.tight_layout(); plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3) Deterministic regression (MSE)\n", "\n", "A basic MLP predicts a single \\( \\hat y(x) \\). This optimizes the **mean-squared error** and **cannot** represent multi-modality or heteroscedasticity."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["class MLP(nn.Module):\n", "    def __init__(self, in_dim=1, out_dim=1, width=128):\n", "        super().__init__()\n", "        self.net = nn.Sequential(\n", "            nn.Linear(in_dim, width), nn.ReLU(),\n", "            nn.Linear(width, width), nn.ReLU(),\n", "            nn.Linear(width, out_dim),\n", "        )\n", "    def forward(self, x):\n", "        return self.net(x)\n", "\n", "mse_model = MLP().to(device)\n", "opt = optim.Adam(mse_model.parameters(), lr=1e-3)\n", "\n", "for ep in range(800):\n", "    for xb, yb in batches(x_t, y_t, bs=256):\n", "        pred = mse_model(xb)\n", "        loss = F.mse_loss(pred, yb)\n", "        opt.zero_grad(); loss.backward(); opt.step()\n", "    if (ep+1) % 200 == 0:\n", "        print(f\"[MSE] epoch {ep+1} loss={loss.item():.4f}\")\n", "\n", "with torch.no_grad():\n", "    mean = mse_model(xg_t).squeeze(-1)\n", "plot_regression(x_t, y_t, xg_t, mean, None, title=\"Deterministic MSE regression\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4) Gaussian regression (NLL)\n", "\n", "Network outputs **mean** and **log-std**; we train by **negative log-likelihood**:\n", "\\[\n", "\\mathcal{L} = -\\frac{1}{B}\\sum_{i=1}^B \\log \\mathcal N\\big(y_i;\\ \\mu(x_i), \\sigma(x_i)^2\\big).\n", "\\]\n", "This captures **heteroscedastic** noise but remains **unimodal**."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["class GaussianReg(nn.Module):\n", "    def __init__(self, in_dim=1, width=128):\n", "        super().__init__()\n", "        self.net = nn.Sequential(\n", "            nn.Linear(in_dim, width), nn.ReLU(),\n", "            nn.Linear(width, width), nn.ReLU(),\n", "            nn.Linear(width, 2)  # output [mu, log_std]\n", "        )\n", "    def forward(self, x):\n", "        h = self.net(x)\n", "        mu = h[..., :1]\n", "        log_std = h[..., 1:]\n", "        std = F.softplus(log_std) + 1e-4  # keep positive and stable\n", "        return mu, std\n", "\n", "def gaussian_nll(mu, std, y):\n", "    dist = D.Normal(loc=mu.squeeze(-1), scale=std.squeeze(-1))\n", "    return -dist.log_prob(y.squeeze(-1)).mean()\n", "\n", "g_model = GaussianReg().to(device)\n", "opt = optim.Adam(g_model.parameters(), lr=1e-3)\n", "\n", "for ep in range(800):\n", "    for xb, yb in batches(x_t, y_t, bs=256):\n", "        mu, std = g_model(xb)\n", "        loss = gaussian_nll(mu, std, yb)\n", "        opt.zero_grad(); loss.backward(); opt.step()\n", "    if (ep+1) % 200 == 0:\n", "        print(f\"[Gaussian] epoch {ep+1} nll={loss.item():.4f}\")\n", "\n", "with torch.no_grad():\n", "    mu, std = g_model(xg_t)\n", "plot_regression(x_t, y_t, xg_t, mu.squeeze(-1), std.squeeze(-1), title=\"Gaussian NLL regression (mean \u00b1 2\u03c3)\")\n", "\n", "@torch.no_grad()\n", "def logp_gaussian(X, Y):\n", "    mu, std = g_model(X)  # [G,1], [G,1]\n", "    dist = D.Normal(mu, std)\n", "    return dist.log_prob(Y)\n", "\n", "plot_density(xg_t, y_lims=(-4,4), steps=200, logp_fn=logp_gaussian, title=\"Learned p(y|x) \u2014 Gaussian\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5) Mixture Density Network (Gaussian Mixture)\n", "\n", "The network outputs parameters of a \\(K\\)-component Gaussian mixture:\n", "- logits \\(\\pi_k(x)\\),\n", "- means \\(\\mu_k(x)\\),\n", "- scales \\(\\sigma_k(x)\\).\n", "\n", "We still train by **negative log-likelihood** (log-sum-exp under the hood), which lets us fit **multi-modal** \\(p(y\\mid x)\\)."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["class MDN(nn.Module):\n", "    def __init__(self, in_dim=1, K=3, width=128):\n", "        super().__init__()\n", "        self.K = K\n", "        self.shared = nn.Sequential(\n", "            nn.Linear(in_dim, width), nn.ReLU(),\n", "            nn.Linear(width, width), nn.ReLU(),\n", "        )\n", "        self.logits = nn.Linear(width, K)\n", "        self.means  = nn.Linear(width, K)\n", "        self.scales = nn.Linear(width, K)  # we'll softplus\n", "    def forward(self, x):\n", "        h = self.shared(x)\n", "        logits = self.logits(h)\n", "        means = self.means(h)\n", "        scales = F.softplus(self.scales(h)) + 1e-4\n", "        return logits, means, scales\n", "\n", "def mdn_nll(logits, means, scales, y):\n", "    # y: [B,1]; params: [B,K]\n", "    comp = D.Normal(loc=means, scale=scales)                # [B,K]\n", "    log_probs = comp.log_prob(y)                            # [B,K]\n", "    log_mix = torch.log_softmax(logits, dim=-1) + log_probs # [B,K]\n", "    log_px = torch.logsumexp(log_mix, dim=-1)               # [B,1]\n", "    return -log_px.mean()\n", "\n", "mdn = MDN(K=5).to(device)\n", "opt = optim.Adam(mdn.parameters(), lr=1e-3)\n", "\n", "for ep in range(1200):\n", "    for xb, yb in batches(x_t, y_t, bs=256):\n", "        logits, means, scales = mdn(xb)\n", "        loss = mdn_nll(logits, means, scales, yb)\n", "        opt.zero_grad(); loss.backward(); opt.step()\n", "    if (ep+1) % 300 == 0:\n", "        print(f\"[MDN] epoch {ep+1} nll={loss.item():.4f}\")\n", "\n", "@torch.no_grad()\n", "def logp_mdn(X, Y):\n", "    logits, means, scales = mdn(X)\n", "    comp = D.Normal(loc=means, scale=scales)               # [G,K]\n", "    log_probs = comp.log_prob(Y)                           # [G,K]\n", "    log_mix = torch.log_softmax(logits, dim=-1) + log_probs\n", "    return torch.logsumexp(log_mix, dim=-1)                # [G,1]\n", "\n", "plot_density(xg_t, y_lims=(-4,4), steps=300, logp_fn=logp_mdn, title=\"Learned p(y|x) \u2014 MDN (K=5)\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6) Draw samples from the predictive distributions\n", "\n", "We visualize **samples** from the learned \\(p(y\\mid x)\\) at a few \\(x\\) locations to highlight multi-modality."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["@torch.no_grad()\n", "def sample_gaussian(x_points, n_samples=200):\n", "    X = torch.tensor(x_points, dtype=torch.float32, device=device).view(-1,1)\n", "    mu, std = g_model(X)\n", "    dist = D.Normal(mu, std)\n", "    return dist.sample((n_samples,)).cpu().numpy().squeeze(-1)  # [S,G]\n", "\n", "@torch.no_grad()\n", "def sample_mdn(x_points, n_samples=200):\n", "    X = torch.tensor(x_points, dtype=torch.float32, device=device).view(-1,1)\n", "    logits, means, scales = mdn(X)          # [G,K]\n", "    mix = D.Categorical(logits=logits)      # [G]\n", "    k = mix.sample((n_samples,))            # [S,G]\n", "    comp = D.Normal(means, scales)          # [G,K]\n", "    # Gather selected component parameters for each sample\n", "    idx = k.unsqueeze(-1)                   # [S,G,1]\n", "    sel_means  = torch.gather(means.unsqueeze(0).expand(n_samples,-1,-1), 2, idx).squeeze(-1)   # [S,G]\n", "    sel_scales = torch.gather(scales.unsqueeze(0).expand(n_samples,-1,-1), 2, idx).squeeze(-1) # [S,G]\n", "    eps = torch.randn_like(sel_means)\n", "    samp = sel_means + sel_scales * eps\n", "    return samp.cpu().numpy()  # [S,G]\n", "\n", "xs = np.array([-3.0, -1.0, 0.0, 1.5, 3.0], dtype=np.float32)\n", "\n", "s_gauss = sample_gaussian(xs, n_samples=400)\n", "s_mdn   = sample_mdn(xs, n_samples=400)\n", "\n", "plt.figure(figsize=(10,3.5))\n", "for i, xv in enumerate(xs):\n", "    plt.subplot(1,len(xs),i+1)\n", "    plt.hist(s_gauss[:,i].reshape(-1), bins=30, density=True, alpha=0.7)\n", "    plt.title(f\"Gaussian @ x={xv:.1f}\")\n", "plt.tight_layout(); plt.show()\n", "\n", "plt.figure(figsize=(10,3.5))\n", "for i, xv in enumerate(xs):\n", "    plt.subplot(1,len(xs),i+1)\n", "    plt.hist(s_mdn[:,i].reshape(-1), bins=30, density=True, alpha=0.7)\n", "    plt.title(f\"MDN @ x={xv:.1f}\")\n", "plt.tight_layout(); plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7) Summary\n", "- **MSE** learns a point estimate and blurs multi-modal targets.\n", "- **Gaussian NLL** learns mean **and** uncertainty, modeling heteroscedasticity but still one mode.\n", "- **MDN (mixture NLL)** captures **multi-modality** and sharp conditional densities.\n", "\n", "All training used **maximum likelihood** (minimizing negative log-probability)."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}